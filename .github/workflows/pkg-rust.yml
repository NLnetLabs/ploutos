# GitHub Actions workflow for building and testing O/S packages.
#
# Workflow speed:
# ===============
# This workflow uses GitHub Actions caching to avoid rebuilding Rust cargo-deb, cargo generate-rpm and our compiled
# dependencies on every run. At the time of writing the GH cache contents expire after a week if not used so the next
# build may be much slower as it will have to re-download/build/install lots of Rust crates.
#
# This workflow does NOT use the lewagon/wait-on-check-action GitHub Action to enable individual sub-jobs of a matrix
# job to wait only for their corresponding "upstream" matrix sub-job (e.g. DEB packaging for x86-64 doesn't need to
# wait on cross-compilation while DEB packaging for ARMv7 does need to wait but only for the ARMv7 cross-compile but
# not for the AARCH64 cross-compile). It was tried, it was great, it speeds up the workflow, but it was unreliable.
# There are known issues with the GitHub Action and having a fragile workflow fail sporadically because the wait
# wrongly proceeds to the next step before the dependent step completed successfully is not acceptable. Perhaps this
# will be better later, or an alternative approach exists that has not yet been found.
#
# DEB/RPM packaging:
# ==================
# DEB and RPM packages are built inside Docker containers as GH Runners have extra libraries and packages installed
# which can cause package building to succeed but package installation on a real target O/S to fail, due to being built
# against too recent version of a package such as libssl or glibc.
#
# Packages are built using the cargo deb and cargo generate-rpm projects, not using official Debian and RedHat tools.
# This allows us to keep the configuration in `Cargo.toml` but can mean that we sometimes hit limitations of those
# tools (e.g. we contributed systemd unit activation support to cargo deb to overcome that lacking capability).
#
# DEB/RPM testing:
# ================
# DEB and RPM packages are tested inside LXC/LXD (now Incus) containers because Docker containers don't by default
# support init managers such as systemd but we want to test systemd service unit installation and activation.
#
# RHEL 8/CentOS 8 support:
# ========================
# We were building with the now discontinued CentOS 8. We continue to build them in a CentOS 8 Docker image but install
# packages from the CentOS 8 vault to work around the forced breakage introduced by RedHat. For testing we were forced
# to switch to using a Rocky Linux (CentOS 8 compatible) LXC/LXD image because the CentOS 8 LXC/LXD image was pulled
# from the LXC/LXD image repositories. In future we may want to explicitly build for Rocky Linux instead or as well as
# CentOS 8.
#
# Docker packaging:
# =================
# Docker packaging was originally done using Docker Hub but long delays and repeated spurious failures caused us to
# migrate Docker packaging to GitHub Actions and which is now part of this workflow.
#
# Images use an Alpine base image for reduced image size and thus download time, and also for faster and simpler
# installation of dependencies (apk add is way faster and simpler than apt-get install for example). However Alpine is
# MUSL based rather than GLIBC based, so cross-compiled binaries (see below) must target MUSL when intending to be
# used within a Docker container.
#
# Images are built using Docker Buildkit (officially supported by Docker and used by default by the Docker Build Push
# GitHub Action) which speeds up especially the non-x86-64 architecture case.
#
# Per architecture images are built and pushed to Docker Hub with xxx-<arch> tags, and then a Docker Manifest is
# created which groups these images into a single multi-arch image without the -<arch> extension on the tag. The
# manifest is then also pushed to Docker Hub. In the case of a release tag (a "v*" Git tag that does not contain a
# dash "-" character, i.e. is not a release candidate e.g. v0.1.2-rc3) two manifests are pushed: one for the release
# tag, e.g. nlnetlabs/<app>:v0.1.2; and one for the "latest" tag, e.g. "nlnetlabs/<app>:latest" (to enable Docker users
# to just run `docker run nlnetlabs/<app>` and get the latest stable release without having to know/specify the actual
# version).
#
# Building of both x86-64 and non-x86-64 architecture images are handled by a single Dockerfile which supports two
# modes of operation. In the default 'build' mode our app is compiled within the Docker container and only the final
# artifacts are kept in the final Docker image. In the alternate 'copy' mode our binaries are copied into the
# build container and compilation within the container is skipped.
#
# Multi-arch image creation is NOT done using Docker Buildkit multi-arch support because (a) that does not support
# configuring the different invocations of the Dockerfile differently (e.g. with MODE=copy for the non-x86-64 cases
# and providing different the binaries to copy in to the image in each case) and (b) because it compiles our app in
# parallel for each architecture at once on a single GitHub Actions runner host which is VERY SLOW [1] even for just a
# couple of architectures. Instead we leverage the GitHub Actions matrix building support to build each image in
# parallel. This means however that we have to manually invoke the `docker manifest` command as it is not handled
# automagically for us.
#
# [1]: https://github.com/moby/buildkit/blob/master/docs/multi-platform.md#builds-are-very-slow-through-emulation
#
# Docker authentication:
# ======================
# Publication to Docker Hub depends on a Docker Hub username and access token being available in the GitHub secrets
# available to this workflow.
#
# Package publication:
# ====================
# Docker packages are published immediately to Docker Hub. DEB and RPM packages are only available to NLnet Labs team
# members with access to the workflow artifacts. Publication of DEB and RPM packages to packages.nlnetlabs.nl requires
# that the separate packaging process outside of GitHub be invoked manually.
#
# Non-x86-64 packaging:
# ====================
# This workflow uses the Rust Tools team Cargo Cross project to cross-compile for architectures other than x86 64, e.g.
# ARMv7/armhf and ARM64/aarch64.
#
# Note: Different tools (rustc, QEmu, Docker, etc) use slightly different names for these targets which can be
# confusing.
#
# Cross-compilation is NOT done using the support built-in to Cargo because this requires for each target architecture
# that you manually install the appropriate toolchain, set the appropriate environment variables, install the
# appropriate strip tool, and store with the source code a .cargo/config.toml file telling Cargo which tool paths to
# use for which architecture. However this comes with a couple of limitations: (a) Cargo Cross itself uses Docker and
# has known issues running Docker-in-Docker from within a Docker container (which is how what was the main job of this
# workflow runs), (b) we are "limited" to base images/architectures supported by Cargo Cross (but there are quite
# a few of these) and (c) if the base Cargo Cross image doesn't include packages or tooling needed by `cargo build`
# then building will fail. For these reasons cross compilation is done as a pre-job in this workflow (so that it can
# run on the GitHub runner Host rather than inside a Docker container).
#
# Non-x86-64 testing:
# ===================
# Only x86-64 architecture packages are sanity checked. Non-x86-64 architecture packages are built but not tested as
# the binaries won't run on the x86-64 GitHub runner host, Docker or LXC/LXD containers. It might be possible to use
# QEmu for this but that is not done at this time.
#
# Artifacts:
# ==========
# The output of this workflow is two-fold:
#   - Images pushed directly to Docker Hub.
#   - Artifacts are uploaded to GitHub on workflow completion and appear as-if attached to the workflow run.
# The latter are consumed by the separate manual external process for publishing to packages.nlnetlabs.nl.
#
# This workflow also uses artifacts internally to pass cross-compiled binares from one workflow job to another.
#   - Cross-compiled binary artifacts are uploaded to GitHub by the 'cross' job.
#   - Both the 'pkg' and 'docker' jobs download these cross-compiled binary artifacts for inclusion in the
#     packages they create.
# Such 'internal' artifacts are named with a 'tmp-' prefix and are ignored by the separate manual external process
# for publishing to packages.nlnetlabs.nl.

name: Packaging

# Designate this workflow as a GitHub Actions "reusable" workflow.
# See: https://docs.github.com/en/actions/using-workflows/reusing-workflows
on:
  workflow_call:
    inputs:
      runs_on:
        description: "An optional runner label to direct GitHub Actions to your desired (e.g. self-hosted) runner. Defaults to ubuntu-22.04."
        required: false
        type: string
        # Don't use 'ubuntu-latest' as that could causes unexpected changes to occur when it is repointed at a newer
        # O/S release.
        default: 'ubuntu-22.04'
      cross_runs_on:
        description: "An optional runner label to direct GitHub Actions to your desired (e.g. self-hosted) runner from cross-compilation jobs. Defaults to the value of runs_on."
        required: false
        type: string
        default: ''

      artifact_prefix:
        description: "An optional prefix to apply to generated artifact names. Intended for internal testing."
        required: false
        type: string
        default: ''
      cross_build_rules:
        description: "This input is deprecated. Cross build targets will be determined automatically."
        required: false
        type: string
        default: ''
      cross_max_wait_mins:
        description: "The maximum number of minutes the pkg and/or docker builds will wait for the cross-compiled artifact to become available. Defaults to 10 minutes."
        required: false
        type: string
        default: '10'
      strict_mode:
        description: "If true, certain types of potential spurious warning or error that are usually ignored will instead be considered fatal. Defaults to false."
        required: false
        type: boolean
        default: false
      manifest_dir:
        description: "The path to a directory containing the Cargo.toml file to use instead of the Cargo.toml in the root directory of the Git clone."
        required: false
        type: string
        default: ''
      workspace_package:
        description: "If provided, take [package] settings from the Cargo.toml of this member of the workspace (rooted at manifest_dir, if specified)."
        required: false
        type: string
        default: ''

      package_build_rules:
        description: "A relative path to a YAML file, or a YAML string, containing a GitHub Actions matrix with 'pkg' (your app name), Docker 'image' (image <os>:<rel>), 'target' (x86_64, or a Rust target triple), 'extra_build_args' (optional), 'os' (optional) fields. See also: https://doc.rust-lang.org/nightly/rustc/platform-support.html"
        required: false
        type: string
        default: '{}'
      package_test_rules:
          description: "'use_package_build_rules' (default), 'none', or a relative path to a YAML file, or a YAML string, containing a GitHub Actions matrix with 'pkg' (from package_build_rules), LXC 'image' (<dist>:<rel>), 'target' (from package_build_rules) and 'mode' (fresh-install or upgrade-from-published). See also: https://uk.lxd.images.canonical.com/"
          required: false
          type: string
          default: 'use_package_build_rules'

      # Using Docker Hub terminology, for a Docker image named nlnetlabs/krill:v0.1.2-arm64:
      #   - The Organization would be 'nlnetlabs'.
      #   - The Repository would be 'krill'.
      #   - The Tag would be v0.1.2-arm64
      # Collectively I refer to the combination of <org>/<repo>:<tag> as the 'image' name,
      docker_org:
        required: false
        type: string
        default: ''
      docker_repo:
        required: false
        type: string
        default: ''
      docker_context_path:
        description: "Relative path to use as the Docker build context. Defaults to the root of the git clone, i.e. '.'."
        required: false
        type: string
        default: '.'
      docker_file_path:
        description: "Relative path to the Dockerfile to build. Defaults to the Dockerfile in the root of the git clone, i.e. './Dockerfile'."
        required: false
        type: string
        default: './Dockerfile'
      docker_build_rules:
        description: "A relative path to a YAML file, or a YAML string, containing a GitHub Actions matrix with platform, shortname, target (required if mode is copy), mode (optional: build (default) or copy) and cargo_args (optional) fields."
        required: false
        type: string
        default: '{}'
      docker_sanity_check_command:
        description: "A command to run inside the Docker container to sanity check that it is working as expected."
        required: false
        type: string
        default: ''

      deb_extra_build_packages:
        description: "A space separated set of additional Debian packages to install when (not cross) compiling."
        required: false
        type: string
        default: ''

      cross_build_args:
        description: "Extra arguments to cargo build when cross-compiling, e.g. `--features static-openssl`."
        required: false
        type: string
        default: ''

      next_ver_label:
        description: "A tag suffix that denotes an in-development rather than release version, e.g. `dev``."
        required: false
        type: string
        default: dev

      rpm_extra_build_packages:
        description: "A space separated set of additional RPM packages to install when (not cross) compiling."
        required: false
        type: string
        default: ''
      rpm_scriptlets_path:
        description: "The path to a TOML file defining one or more of pre_install_script, post_install_script and/or post_uninstall_script."
        required: false
        type: string
        default: ''

      package_test_scripts_path:
        description: "The path to find scripts for running tests. Invoked scripts take a single argument: post-install or post-upgrade."
        required: false
        type: string
        default: ''

    secrets:
      DOCKER_HUB_ID:
        required: false
      DOCKER_HUB_TOKEN:
        required: false

    outputs:
      docker_images_published:
        description: "Whether publishing of Docker images was done or not."
        value: ${{ jobs.docker.outputs.published }}
      docker_manifest_published:
        description: "Whether publishing of the Docker manifest was done or not."
        value: ${{ jobs.docker-manifest.outputs.published }}

defaults:
  run:
    # see: https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#using-a-specific-shell
    shell: bash --noprofile --norc -eo pipefail -x {0}

env:
  DEBIAN_FRONTEND: noninteractive
  # See: https://github.com/actions/checkout/issues/1809
  ACTIONS_ALLOW_USE_UNSECURE_NODE_VERSION: true

jobs:
  # -------------------------------------------------------------------------------------------------------------------
  # Job: 'prepare'
  # -------------------------------------------------------------------------------------------------------------------
  # Validate and pre-process inputs.
  prepare:
    runs-on: ${{ inputs.runs_on }}
    outputs:
      cargo_name: ${{ steps.verify_inputs.outputs.cargo_name }}
      cargo_package_toml_path: ${{ steps.verify_inputs.outputs.cargo_package_toml_path }}
      cargo_manifest_path_arg: ${{ steps.verify_inputs.outputs.cargo_manifest_path_arg }}
      cargo_read_manifest_path_arg: ${{ steps.verify_inputs.outputs.cargo_read_manifest_path_arg }}
      cargo_workspace_package_arg: ${{ steps.verify_inputs.outputs.cargo_workspace_package_arg }}
      has_docker_secrets: ${{ steps.verify_inputs.outputs.has_docker_secrets }}
      all_repo_and_tag_pairs: ${{ steps.encode.outputs.all_repo_and_tag_pairs }}
      first_repo_and_tag_pair: ${{ steps.encode.outputs.first_repo_and_tag_pair }}
      lower_docker_org: ${{ steps.encode.outputs.lower_docker_org }}
      runs_on: ${{ steps.determine_cross_build_rules.outputs.runs_on }}
      cross_build_rules: ${{ steps.determine_cross_build_rules.outputs.matrix }}
      cross_max_tries: ${{ steps.determine_cross_build_rules.outputs.max_tries }}
      cross_retry_delay_ms: ${{ steps.determine_cross_build_rules.outputs.retry_delay_ms }}
      cross_runs_on: ${{ steps.determine_cross_build_rules.outputs.cross_runs_on }}
      docker_build_rules: ${{ steps.pre_process_rules.outputs.docker_build_rules }}
      package_build_rules: ${{ steps.pre_process_rules.outputs.package_build_rules }}
      package_test_rules: ${{ steps.pre_process_rules.outputs.package_test_rules }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # TODO: extract common code into helper functions
      - name: Pre-process rules
        id: pre_process_rules
        run: |
          if [[ '${{ inputs.cross_build_rules }}' != '' ]]; then
            echo "::warning::The 'cross_build_rules' input is deprecated. Cross build targets will be determined automatically."
          fi

          DOCKER_BUILD_RULES=$(cat <<'EOF'
          ${{ inputs.docker_build_rules }}
          EOF
          )
          if [[ -f "${DOCKER_BUILD_RULES}" ]]; then
            DOCKER_BUILD_RULES_RAW_JSON=$(yq "${DOCKER_BUILD_RULES}" -I=0 -p=yaml -o=json)
          else
            DOCKER_BUILD_RULES_RAW_JSON=$(echo "${DOCKER_BUILD_RULES}" | yq -I=0 -p=yaml -o=json)
          fi

          # Convert string values to array of single string values (as GitHub matrix values must be arrays).
          DOCKER_BUILD_RULES_PROCESSED_JSON=$(echo ${DOCKER_BUILD_RULES_RAW_JSON} | jq 'with_entries(if .value | type == "string" then .value |= [.] elif .value | type != "array" then error("rule values must be strings or arrays") else . end)')

          # Rename "crosstarget" keys to "target" (we support "crosstarget" for backward compatibility).
          DOCKER_BUILD_RULES_PROCESSED_JSON=$(echo ${DOCKER_BUILD_RULES_PROCESSED_JSON} | jq '(.. | select(has("crosstarget")?)) |= with_entries(if .key == "crosstarget" then .key = "target" else . end)')

          echo "docker_build_rules<<END_OF_DOCKER_BUILD_RULES" >> $GITHUB_OUTPUT
          echo ${DOCKER_BUILD_RULES_PROCESSED_JSON} | jq >> $GITHUB_OUTPUT
          echo 'END_OF_DOCKER_BUILD_RULES' >> $GITHUB_OUTPUT

          PACKAGE_BUILD_RULES=$(cat <<'EOF'
          ${{ inputs.package_build_rules }}
          EOF
          )
          if [[ -f "${PACKAGE_BUILD_RULES}" ]]; then
            PACKAGE_BUILD_RULES_RAW_JSON=$(yq "${PACKAGE_BUILD_RULES}" -I=0 -p=yaml -o=json)
          else
            PACKAGE_BUILD_RULES_RAW_JSON=$(echo "${PACKAGE_BUILD_RULES}" | yq -I=0 -p=yaml -o=json)
          fi

          # Convert string values to array of single string values (as GitHub matrix values must be arrays).
          PACKAGE_BUILD_RULES_ARRAYS_JSON=$(echo ${PACKAGE_BUILD_RULES_RAW_JSON} | jq 'with_entries(if .value | type == "string" then .value |= [.] elif .value | type != "array" then error("rule values must be strings or arrays") else . end)')

          # Don't permute the build job over variables intended only for use by the pkg-test job but which were supplied
          # via package_build_rules as a convenience for the user to avoid having to supply package_test_rules which
          # would be mostly identical to package_build_rules. If we don't remove this we end up causing duplicate Build
          # jobs to be spawned for each different test mode (fresh-install vs upgrade-from-published) which for the
          # pkb job are identical and thus pointless waste and just plain confusing.
          PACKAGE_BUILD_RULES_PROCESSED_JSON=$(echo ${PACKAGE_BUILD_RULES_ARRAYS_JSON} | jq -c 'del(."test-mode") | del(.include[]?."test-mode") | del(."test-exclude")')
          PACKAGE_BUILD_RULES_PROCESSED_JSON=$(echo ${PACKAGE_BUILD_RULES_PROCESSED_JSON} | jq -c 'del(."test-image") | del(.include[]?."test-image")')

          # And now also for the older names for backward compatibility.
          PACKAGE_BUILD_RULES_PROCESSED_JSON=$(echo ${PACKAGE_BUILD_RULES_PROCESSED_JSON} | jq -c 'del(.mode) | del(.include[]?.mode)')

          # Exclude centos:7 from building because the CentOS package mirror is no longer available.
          # Specifically:
          #   $ yum update -y
          #   Loaded plugins: fastestmirror, ovl
          #   Determining fastest mirrors
          #   Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=os&infra=container error was
          #   14: curl#6 - "Could not resolve host: mirrorlist.centos.org; Unknown error"
          if [[ "${PACKAGE_BUILD_RULES_PROCESSED_JSON}" != "{}" ]]; then
            CONTROL_JSON=$(echo ${PACKAGE_BUILD_RULES_PROCESSED_JSON} | jq -c)
            MODIFIED_JSON=$(echo ${PACKAGE_BUILD_RULES_PROCESSED_JSON} | jq -c 'del(.image[]? | select(. == "centos:7")) | del(.include[]? | select(.image == "centos:7"))')
            MODIFIED_JSON=$(echo ${MODIFIED_JSON} | jq -c 'del(.include[]? | select(.os == "debian:centos7"))')
            if [[ "${CONTROL_JSON}" != "${MODIFIED_JSON}" ]]; then
              echo "::warning::Removed centos:7 image from package_build_rules because mirrorlist.centos.org is no longer available"
              PACKAGE_BUILD_RULES_PROCESSED_JSON="${MODIFIED_JSON}"
            fi
          fi

          # Exclude ubuntu:xenial as its GLIBC is too old to support Node 20 now required to use GitHub Actions
          if [[ "${PACKAGE_BUILD_RULES_PROCESSED_JSON}" != "{}" ]]; then
            CONTROL_JSON=$(echo ${PACKAGE_BUILD_RULES_PROCESSED_JSON} | jq -c)
            MODIFIED_JSON=$(echo ${PACKAGE_BUILD_RULES_PROCESSED_JSON} | jq -c 'del(.image[]? | select(. == "ubuntu:xenial")) | del(.include[]? | select(.image == "ubuntu:xenial"))')
            MODIFIED_JSON=$(echo ${MODIFIED_JSON} | jq -c 'del(.include[]? | select(.os == "ubuntu:xenial"))')
            if [[ "${CONTROL_JSON}" != "${MODIFIED_JSON}" ]]; then
              echo "::warning::Removed ubuntu:xenial image from package_build_rules because its GLIBC is too old to support Node 20 required by GitHub Actions"
              PACKAGE_BUILD_RULES_PROCESSED_JSON="${MODIFIED_JSON}"
            fi
          fi

          # Exclude ubuntu:bionic as its GLIBC is too old to support Node 20 now required to use GitHub Actions
          if [[ "${PACKAGE_BUILD_RULES_PROCESSED_JSON}" != "{}" ]]; then
            CONTROL_JSON=$(echo ${PACKAGE_BUILD_RULES_PROCESSED_JSON} | jq -c)
            MODIFIED_JSON=$(echo ${PACKAGE_BUILD_RULES_PROCESSED_JSON} | jq -c 'del(.image[]? | select(. == "ubuntu:bionic")) | del(.include[]? | select(.image == "ubuntu:bionic"))')
            MODIFIED_JSON=$(echo ${MODIFIED_JSON} | jq -c 'del(.include[]? | select(.os == "ubuntu:bionic"))')
            if [[ "${CONTROL_JSON}" != "${MODIFIED_JSON}" ]]; then
              echo "::warning::Removed ubuntu:bionic image from package_build_rules because its GLIBC is too old to support Node 20 required by GitHub Actions"
              PACKAGE_BUILD_RULES_PROCESSED_JSON="${MODIFIED_JSON}"
            fi
          fi

          # Exclude debian:stretch as its GLIBC is too old to support Node 20 now required to use GitHub Actions
          if [[ "${PACKAGE_BUILD_RULES_PROCESSED_JSON}" != "{}" ]]; then
            CONTROL_JSON=$(echo ${PACKAGE_BUILD_RULES_PROCESSED_JSON} | jq -c)
            MODIFIED_JSON=$(echo ${PACKAGE_BUILD_RULES_PROCESSED_JSON} | jq -c 'del(.image[]? | select(. == "debian:stretch")) | del(.include[]? | select(.image == "debian:stretch"))')
            MODIFIED_JSON=$(echo ${MODIFIED_JSON} | jq -c 'del(.include[]? | select(.os == "debian:stretch"))')
            if [[ "${CONTROL_JSON}" != "${MODIFIED_JSON}" ]]; then
              echo "::warning::Removed debian:stretch image from package_build_rules because its GLIBC is too old to support Node 20 required by GitHub Actions"
              PACKAGE_BUILD_RULES_PROCESSED_JSON="${MODIFIED_JSON}"
            fi
          fi

          echo "package_build_rules<<END_OF_PACKAGE_BUILD_RULES" >> $GITHUB_OUTPUT
          echo ${PACKAGE_BUILD_RULES_PROCESSED_JSON} | jq >> $GITHUB_OUTPUT
          echo 'END_OF_PACKAGE_BUILD_RULES' >> $GITHUB_OUTPUT

          PACKAGE_TEST_RULES=$(cat <<'EOF'
          ${{ inputs.package_test_rules }}
          EOF
          )
          if [[ "${PACKAGE_TEST_RULES}" == 'none' ]]; then
            PACKAGE_TEST_RULES_RAW_JSON='{}'
          elif [[ "${PACKAGE_TEST_RULES}" == 'use_package_build_rules' ]]; then
            PACKAGE_TEST_RULES_RAW_JSON="${PACKAGE_BUILD_RULES_ARRAYS_JSON}"
          elif [[ -f "${PACKAGE_TEST_RULES}" ]]; then
            PACKAGE_TEST_RULES_RAW_JSON=$(yq "${PACKAGE_TEST_RULES}" -I=0 -p=yaml -o=json)
          else
            PACKAGE_TEST_RULES_RAW_JSON=$(echo "${PACKAGE_TEST_RULES}" | yq -I=0 -p=yaml -o=json)
          fi

          # Convert string values to array of single string values (as GitHub matrix values must be arrays).
          PACKAGE_TEST_RULES_PROCESSED_JSON=$(echo ${PACKAGE_TEST_RULES_RAW_JSON} | jq 'with_entries(if .value | type == "string" then .value |= [.] elif .value | type != "array" then error("rule values must be strings or arrays") else . end)')

          # Exclude debian:stretch because the LXC image is no longer available.
          if [[ "${PACKAGE_TEST_RULES_PROCESSED_JSON}" != "{}" ]]; then
            CONTROL_JSON=$(echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq -c)
            MODIFIED_JSON=$(echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq -c 'del(.image[]? | select(. == "debian:stretch")) | del(.include[]? | select(.image == "debian:stretch"))')
            MODIFIED_JSON=$(echo ${MODIFIED_JSON} | jq -c 'del(.include[]? | select(.os == "debian:stretch"))')
            if [[ "${CONTROL_JSON}" != "${MODIFIED_JSON}" ]]; then
              echo "::warning::Removed debian:stretch image from package_test_rules because the LXC/Incus image no longer exists"
              PACKAGE_TEST_RULES_PROCESSED_JSON="${MODIFIED_JSON}"
            fi
          fi

          # Exclude ubuntu:bionic because the LXC image is no longer available.
          if [[ "${PACKAGE_TEST_RULES_PROCESSED_JSON}" != "{}" ]]; then
            CONTROL_JSON=$(echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq -c)
            MODIFIED_JSON=$(echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq -c 'del(.image[]? | select(. == "ubuntu:bionic")) | del(.include[]? | select(.image == "ubuntu:bionic"))')
            MODIFIED_JSON=$(echo ${MODIFIED_JSON} | jq -c 'del(.include[]? | select(.os == "ubuntu:bionic"))')
            if [[ "${CONTROL_JSON}" != "${MODIFIED_JSON}" ]]; then
              echo "::warning::Removed ubuntu:bionic image from package_test_rules because the LXC/Incus image no longer exists"
              PACKAGE_TEST_RULES_PROCESSED_JSON="${MODIFIED_JSON}"
            fi
          fi

          # Exclude centos:7 because we excluded it from building above.
          if [[ "${PACKAGE_TEST_RULES_PROCESSED_JSON}" != "{}" ]]; then
            CONTROL_JSON=$(echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq -c)
            MODIFIED_JSON=$(echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq -c 'del(.image[]? | select(. == "centos:7")) | del(.include[]? | select(.image == "centos:7"))')
            MODIFIED_JSON=$(echo ${MODIFIED_JSON} | jq -c 'del(.include[]? | select(.os == "centos:7"))')
            if [[ "${CONTROL_JSON}" != "${MODIFIED_JSON}" ]]; then
              echo "::warning::Removed centos:7 image from package_test_rules because Ploutos no longer supports building for CentOS 7"
              PACKAGE_TEST_RULES_PROCESSED_JSON="${MODIFIED_JSON}"
            fi
          fi

          # Exclude ubuntu:xenial because the LXC image is no longer available.
          if [[ "${PACKAGE_TEST_RULES_PROCESSED_JSON}" != "{}" ]]; then
            CONTROL_JSON=$(echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq -c)
            MODIFIED_JSON=$(echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq -c 'del(.image[]? | select(. == "ubuntu:xenial")) | del(.include[]? | select(.image == "ubuntu:xenial"))')
            MODIFIED_JSON=$(echo ${MODIFIED_JSON} | jq -c 'del(.include[]? | select(.os == "ubuntu:xenial"))')
            if [[ "${CONTROL_JSON}" != "${MODIFIED_JSON}" ]]; then
              echo "::warning::Removed ubuntu:xenial image from package_test_rules because the LXC/Incus image no longer exists"
              PACKAGE_TEST_RULES_PROCESSED_JSON="${MODIFIED_JSON}"
            fi
          fi

          # Exclude non-x86_64 targets as we only run the tests on x86-64 GitHub runners.
          if [[ "${PACKAGE_TEST_RULES_PROCESSED_JSON}" != "{}" ]]; then
            CONTROL_JSON=$(echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq -c)
            MODIFIED_JSON=$(echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq -c 'del(.target[]? | select(. != "x86_64")) | del(.include[]? | select(.target != "x86_64" and .target != null))')
            if [[ "${CONTROL_JSON}" != "${MODIFIED_JSON}" ]]; then
              echo "::warning::Removed non-x86_64 targets from package_test_rules because testing is only supported for x86_64 targets"
              PACKAGE_TEST_RULES_PROCESSED_JSON="${MODIFIED_JSON}"
            fi
          fi

          # In the test rules, rename "test-mode" key to "mode" and "test-exclude" to "exclude".
          # 'exclude' is used by GitHub Actions itself, while 'mode' has meaning for us. We're renaming them because
          # prefixing them with 'test-' when used in the build rules makes it clearer that they relate to generated
          # default test rules.
          PACKAGE_TEST_RULES_PROCESSED_JSON=$(echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq '(.. | select(has("test-mode")?)) |= with_entries(if .key == "test-mode" then .key = "mode" else . end)')
          PACKAGE_TEST_RULES_PROCESSED_JSON=$(echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq '(.. | select(has("test-exclude")?)) |= with_entries(if .key == "test-exclude" then .key = "exclude" else . end)')

          echo "package_test_rules<<END_OF_PACKAGE_TEST_RULES" >> $GITHUB_OUTPUT
          echo ${PACKAGE_TEST_RULES_PROCESSED_JSON} | jq >> $GITHUB_OUTPUT
          echo 'END_OF_PACKAGE_TEST_RULES' >> $GITHUB_OUTPUT

      - name: Determine cross build rules
        id: determine_cross_build_rules
        run: |
          JSON='${{ steps.pre_process_rules.outputs.docker_build_rules }}'
          DOCKER_TARGETS=$(echo $JSON | jq '.. | .target? | select(. != null)')

          JSON='${{ steps.pre_process_rules.outputs.package_build_rules }}'
          PKG_TARGETS=$(echo $JSON | jq '.. | .target? | select(. != null)')

          # This is a bit ridiculous invoking jq three times... there must be an easier way
          ALL_TARGETS=$(echo ${DOCKER_TARGETS} ${PKG_TARGETS} | jq -s 'flatten | unique | .[] | select(. != "x86_64")' | jq -s)

          RUNS_ON="\"${{ inputs.runs_on }}\""

          if [ "${{ inputs.cross_runs_on }}" == '' ]; then
            CROSS_RUNS_ON="${RUNS_ON}"
          else
            CROSS_RUNS_ON="\"${{ inputs.cross_runs_on }}\""
          fi

          echo "matrix<<END_OF_TARGETS" >> $GITHUB_OUTPUT
          if [ "${ALL_TARGETS}" == '[]' ]; then
            echo '{}' >> $GITHUB_OUTPUT
          else
            echo '{ "target": ' ${ALL_TARGETS} '}' | jq >> $GITHUB_OUTPUT
          fi
          echo "END_OF_TARGETS" >> $GITHUB_OUTPUT
          
          echo "runs_on<<END_OF_RUNS_ON" >> $GITHUB_OUTPUT
          echo '{ "labels": [' ${RUNS_ON} '] }' | jq >> $GITHUB_OUTPUT
          echo "END_OF_RUNS_ON" >> $GITHUB_OUTPUT
    
          echo "cross_runs_on<<END_OF_CROSS_RUNS_ON" >> $GITHUB_OUTPUT
          echo '{ "labels": [' ${CROSS_RUNS_ON} '] }' | jq >> $GITHUB_OUTPUT
          echo "END_OF_CROSS_RUNS_ON" >> $GITHUB_OUTPUT

          RETRY_DELAY_MS=30000
          MAX_TRIES=$(( ( ${{ inputs.cross_max_wait_mins }} * 60 * 1000 ) / ${RETRY_DELAY_MS} ))
          echo "retry_delay_ms=${RETRY_DELAY_MS}" >> $GITHUB_OUTPUT
          echo "max_tries=${MAX_TRIES}" >> $GITHUB_OUTPUT

      - name: Print rules
        # Disable default use of bash -x for easier to read output in the log
        shell: bash
        run: |
          echo "============================================================================="
          echo "Cross build rules:"
          echo "============================================================================="
          JSON='${{ steps.determine_cross_build_rules.outputs.matrix }}'
          if [[ "${JSON}" != "{}" ]]; then
            echo '${{ steps.determine_cross_build_rules.outputs.matrix }}'
          else
            echo None
          fi

          echo
          echo "============================================================================="
          echo "Docker build rules:"
          echo "============================================================================="
          JSON='${{ steps.pre_process_rules.outputs.docker_build_rules }}'
          if [[ "${JSON}" != "{}" ]]; then
            echo '${{ steps.pre_process_rules.outputs.docker_build_rules }}'
          else
            echo None
          fi

          echo
          echo "============================================================================="
          echo "Package build rules:"
          echo "============================================================================="
          JSON='${{ steps.pre_process_rules.outputs.package_build_rules }}'
          if [[ "${JSON}" != "{}" ]]; then
            echo '${{ steps.pre_process_rules.outputs.package_build_rules }}'
          else
            echo None
          fi

          echo
          echo "============================================================================="
          echo "Packages test rules:"
          echo "============================================================================="
          JSON='${{ steps.pre_process_rules.outputs.package_test_rules }}'
          if [[ "${JSON}" != "{}" ]]; then
            echo '${{ steps.pre_process_rules.outputs.package_test_rules }}'
          else
            echo None
          fi

      - name: Verify inputs
        id: verify_inputs
        run: |
          CARGO_MANIFEST_PATH_ARG=""
          if [[ "${{ inputs.manifest_dir}}" != '' ]]; then
            CARGO_MANIFEST_PATH_ARG="--manifest-path ${{ inputs.manifest_dir }}/Cargo.toml"
          fi
          echo "cargo_manifest_path_arg=${CARGO_MANIFEST_PATH_ARG}" >> $GITHUB_OUTPUT

          CARGO_WORKSPACE_PACKAGE_ARG=""
          if [[ "${{ inputs.workspace_package}}" != '' ]]; then
            CARGO_WORKSPACE_PACKAGE_ARG="--package ${{ inputs.workspace_package }}"
          fi
          echo "cargo_workspace_package_arg=${CARGO_WORKSPACE_PACKAGE_ARG}" >> $GITHUB_OUTPUT

          CARGO_PACKAGE_TOML_PATH=""
          CARGO_READ_MANIFEST_PATH_ARG=""
          if [[ '${{ inputs.manifest_dir }}' != '' ]]; then
            CARGO_PACKAGE_TOML_PATH="${{ inputs.manifest_dir }}/"
          fi
          if [[ '${{ inputs.workspace_package}}' != '' ]]; then
            CARGO_PACKAGE_TOML_PATH="${CARGO_PACKAGE_TOML_PATH}${{ inputs.workspace_package}}/"
          fi
          CARGO_PACKAGE_TOML_PATH="${CARGO_PACKAGE_TOML_PATH}Cargo.toml"

          if [[ "${CARGO_PACKAGE_TOML_PATH}" != "Cargo.toml" ]]; then
            CARGO_READ_MANIFEST_PATH_ARG="--manifest-path ${CARGO_PACKAGE_TOML_PATH}"
          fi

          echo "cargo_package_toml_path=${CARGO_PACKAGE_TOML_PATH}" >> $GITHUB_OUTPUT
          echo "cargo_read_manifest_path_arg=${CARGO_READ_MANIFEST_PATH_ARG}" >> $GITHUB_OUTPUT

          CARGO_NAME=$(cat ${CARGO_PACKAGE_TOML_PATH} | docker run --rm -i sclevine/yj -tj | jq -r .package.name)
          echo "cargo_name=${CARGO_NAME}" >> $GITHUB_OUTPUT

          if [[ '${{ inputs.next_ver_label }}' == '' ]]; then
            echo "::error::Workflow input 'next_ver_label' must be non-empty if set."
            exit 1
          fi

          if [[ '${{ inputs.rpm_scriptlets_path }}' != '' ]]; then
            if [[ ! -f '${{ inputs.rpm_scriptlets_path }}' ]]; then
              echo "::error::Workflow input 'rpm_scriptlets_path' ('${{ inputs.rpm_scriptlets_path }}') must refer to a file in the Git checkout"
              exit 1
            fi
          fi

          if [[ '${{ steps.pre_process_rules.outputs.docker_build_rules }}' != '{}' ]]; then
            if [[ '${{ inputs.docker_org }}' == '' ]]; then
              echo "::error::Workflow input 'docker_org' must be non-empty when 'docker_build_rules' are supplied."
              exit 1
            fi

            if [[ '${{ inputs.docker_repo }}' == '' ]]; then
              echo "::error::Workflow input 'docker_repo' must be non-empty when 'docker_build_rules' are supplied."
              exit 1
            fi
          fi

          if [[ "${{ secrets.DOCKER_HUB_ID }}" != '' || "${{ secrets.DOCKER_HUB_TOKEN }}" != '' ]]; then
            echo "has_docker_secrets=true" >> $GITHUB_OUTPUT
          else
            echo "has_docker_secrets=false" >> $GITHUB_OUTPUT
          fi

      - name: Verify Docker credentials
        if: ${{ steps.pre_process_rules.outputs.docker_build_rules != '{}' && fromJSON(steps.verify_inputs.outputs.has_docker_secrets) == true }}
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_HUB_ID }}
          password: ${{ secrets.DOCKER_HUB_TOKEN }}

      # NOTE: This step does NOT actually tag anything in Docker, either locally or on Docker Hub, it only generates a
      # potential set of string tag values that can be used later in the workflow.
      #
      # Based on the Git ref, e.g. a tag or branch, determine the appropriate tag(s) for the Docker image we will
      # create, e.g. <app>:unstable for a main branch commit, or <app>:v1.2.3 _and_ <app>:latest for a release tag, or
      # <app>:test for anything else. The action also determines a set of potential Docker labels that we might wish to
      # apply to the Docker image, e.g. (these are the actual values the action produced for the v0.10.0 Krill release
      # tag):
      #     org.opencontainers.image.title=krill
      #     org.opencontainers.image.description=RPKI Certificate Authority and Publication Server written in Rust
      #     org.opencontainers.image.url=https://github.com/NLnetLabs/krill
      #     org.opencontainers.image.source=https://github.com/NLnetLabs/krill
      #     org.opencontainers.image.version=v0.10.0
      #     org.opencontainers.image.created=2022-09-05T14:52:15.182Z
      #     org.opencontainers.image.revision=2c00aa05e2299ca8a0994f7d054231e3a5cd8d25
      #     org.opencontainers.image.licenses=MPL-2.0
      #
      # The results of the action are available to subsequent steps via steps.meta.output.tags and
      # steps.meta.output.labels (both line-break separated).
      #
      # The rules defined for the docker/metadata-action below are as follows, assuming:
      #   with:
      #     images: <app>
      # 
      # On push of a Git tag to refs/tags/v1.2.3 the Docker tags will be '<app>:v1.2.3' and '<app>:latest'
      # because of:
      #   type=semver,pattern={{version}},prefix=v
      #   type=raw,value=latest,enable=${{ github.ref != 'refs/heads/main' && !contains(github.ref, '-') }}
      #                                    ^^^^^^^^^^^^^ true, not main       ^^^^^^^^^ true, no dash found
      #
      #   Note: we don't use semver,pattern={{raw}} because while that preserves the leading v in v1.2.3 it
      #   discards the leading v in v1.2.3-rc4.
      #
      # On push of a Git tag to refs/tags/v1.2.3-rc1 the Docker tags will be '<app>:v1.2.3' but NOT '<app>:latest'
      # because of:
      #   type=semver,pattern={{raw}}
      #   type=raw,value=latest,enable=${{ github.ref != 'refs/heads/main' && !contains(github.ref, '-') }}
      #                                    ^^^^^^^^^^^^^ true, not main       ^^^^^^^^^ false, dash found
      #
      # On push to Git refs/heads/main the Docker tag will be '<app>:unstable' because of:
      #   type=raw,value=unstable,enable=${{ github.ref == 'refs/heads/main' }}
      #
      # We set flavor latest=false to disable the default automatic output of a "latest" tag as there are cases
      # where a Git tag was pushed but we do NOT want to generate a "latest" tag, e.g. for release candidates,
      # instead we configure the docker/metadata-action with our own rule determining when to output a "latest"
      # tag.
      - name: Apply rules to Git metadata to generate potential Docker tags
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ inputs.docker_repo }}
          flavor: |
            latest=false
          tags: |
            type=semver,pattern={{version}},prefix=v
            type=raw,value=unstable,enable=${{ github.ref == 'refs/heads/main' }}
            type=raw,value=latest,enable=${{ github.ref != 'refs/heads/main' && !contains(github.ref, '-') }}
            type=raw,value=test,enable=${{ !contains(github.ref, 'refs/tags/v') && github.ref != 'refs/heads/main' }}

      # Encode values as base64 to avoid GitHub Actions refusing to pass the output on to the job that needs it with
      # warning "Skip output '...' since it may contain secret.". This can happen if the docker_org value contains the
      # DOCKER_HUB_ID value. E.g. if docker_org were 'nlnetlabs' and the user to login to Docker Hub as is also
      # 'nlnetlabs' then Docker thinks the latter, a secret, is being leaked via the workflow output defined above.
      # We disable output wrapping by the base64 command because the entire value of a key=value pair in the output
      # file must be on a single line, otherwise it results in an "Invalid format" error from GitHub Actions.
      - name: Encode outputs for passing safely to downstream jobs
        id: encode
        run: |
          ENCODED_ALL_REPO_AND_TAG_PAIRS=$(echo "${{ steps.meta.outputs.tags }}" | base64 --wrap=0)
          echo "all_repo_and_tag_pairs=${ENCODED_ALL_REPO_AND_TAG_PAIRS}" >> $GITHUB_OUTPUT

          ENCODED_FIRST_REPO_AND_TAG_PAIR=$(echo "${{ fromJSON(steps.meta.outputs.json).tags[0] }}" | base64 --wrap=0)
          echo "first_repo_and_tag_pair=${ENCODED_FIRST_REPO_AND_TAG_PAIR}" >> $GITHUB_OUTPUT

          ENCODED_LOWER_DOCKER_ORG=$(echo "${{ inputs.docker_org }}" | tr '[:upper:]' '[:lower:]' | base64 --wrap=0)
          echo "lower_docker_org=${ENCODED_LOWER_DOCKER_ORG}" >> $GITHUB_OUTPUT


  # -------------------------------------------------------------------------------------------------------------------
  # Job: 'cross'
  # -------------------------------------------------------------------------------------------------------------------
  # Cross-compiles packages in a separate job so that we can run it on the GitHub Actions runner host directly rather
  # than inside a Docker container (as is done by the `pkg` job below). We do this because we use `cargo cross` to
  # handle the complexity of using the right compile-time tooling and dependencies for cross compilation to work, and 
  # `cargo cross` works by launching its own Docker container. Trying to launch a Docker container from within a Docker
  # container, the so-called Docker-in-Docker scenario, is more difficult for `cargo cross` to handle correctly and
  # didn't work when I tried it, even with `CROSS_DOCKER_IN_DOCKER=true` set in the environment, hence this approach.
  #
  # See: https://github.com/rust-embedded/cross#docker-in-docker
  cross:
    if: ${{ needs.prepare.outputs.cross_build_rules != '{}' }}
    needs: prepare
    runs-on: ${{ fromJSON(needs.prepare.outputs.cross_runs_on) }}
    strategy:
      matrix: ${{ fromJSON(needs.prepare.outputs.cross_build_rules) }}
    steps:
    - name: Skip if no cross-compilation required
      id: skip
      run: |
        if [[ "${{ matrix.target }}" == "skip" ]]; then
          echo "skip=true" >> $GITHUB_OUTPUT
        else
          echo "skip=false" >> $GITHUB_OUTPUT
        fi

    - name: Checkout repository
      if: ${{ steps.skip.skip != 'true' }}
      uses: actions/checkout@v4

    # Install Rust the hard way rather than using a GH Action because the action doesn't work inside a Docker container.
    # Cargo cross "requires a rustup installation of Rust", an O/S package provided Rust won't
    - name: Install Rust
      if: ${{ steps.skip.skip != 'true' }}
      id: rust
      run: |
        case ${OS_NAME} in
          debian|ubuntu)
            apt-get update
            apt-get install -y curl
            ;;
        esac

        curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- --profile minimal -y
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
        source "$HOME/.cargo/env"
        echo "bin_dir=$HOME/.cargo/bin" >> $GITHUB_OUTPUT

    - name: Fetch cargo cross if available
      if: ${{ steps.skip.skip != 'true' }}
      id: cache-cargo-cross
      uses: actions/cache@v3
      with:
        path: |
          ${{ steps.rust.outputs.bin_dir }}/cross
          ${{ steps.rust.outputs.bin_dir }}/cross-util
        key: ${{ matrix.target }}-cargo-cross

    - name: Install cargo cross if needed
      id: install-cargo-cross
      if: ${{ steps.skip.skip != 'true' && steps.cache-cargo-cross.outputs.cache-hit != 'true' }}
      run: |
        cargo install cross
        echo "installed=true" >> $GITHUB_OUTPUT

    - name: Force cache save
      if: ${{ steps.skip.skip != 'true' && steps.install-cargo-cross.outputs.installed == 'true' }}
      uses: actions/cache/save@v3
      with:
        path: |
          ${{ steps.rust.outputs.bin_dir }}/cross
          ${{ steps.rust.outputs.bin_dir }}/cross-util
        key: ${{ matrix.target }}-cargo-cross

    - name: Ensure ~/.cargo/bin/ is in the path
      if: ${{ steps.skip.skip != 'true' }}
      run: |
        echo $(realpath ~/.cargo/bin) >> $GITHUB_PATH

    - name: Cross compile
      if: ${{ steps.skip.skip != 'true' }}
      run: |
        cross build ${{needs.prepare.outputs.cargo_manifest_path_arg}} ${{needs.prepare.outputs.cargo_workspace_package_arg}} --locked --release -v --target ${{ matrix.target }} ${{ inputs.cross_build_args }}

    - name: Tar the set of created binaries to upload
      if: ${{ steps.skip.skip != 'true' }}
      run: |
        rm -f bins.tar
        find target/${{ matrix.target }}/release/ -maxdepth 1 -type f -executable | xargs tar vpcf bins.tar

    # Upload cross compiled binaries as GitHub Actions artifacts for use by the `pkg` job below. We can't use job
    # outputs as those are limited to 50 MB which we could easily exceed. We can't use actions/cache as cached items
    # are not necessarily available on different operating systems as the cache mechanism uses different namespaces
    # for different compression types and different compression types by operating system. As we don't want these
    # artifacts to be packaged by the scripts that upload to packages.nlnetlabs.nl we prefix the artifact name with
    # `tmp-` which will be ignored by packages.nlnetlabs.nl scripts.
    - name: Upload built binaries
      if: ${{ steps.skip.skip != 'true' }}
      uses: NLnetLabs/upload-artifact@main
      with:
        name: ${{ inputs.artifact_prefix }}tmp-cross-binaries-${{ matrix.target }}
        path: bins.tar
        if-no-files-found: warn

  # -------------------------------------------------------------------------------------------------------------------
  # Job: 'pkg'
  # -------------------------------------------------------------------------------------------------------------------
  # Use the cargo-deb and cargo-generate-rpm Rust crates to build Debian and RPM packages respectively for installing
  # our app. See:
  #   - https://github.com/mmstick/cargo-deb
  #   - https://github.com/cat-in-136/cargo-generate-rpm
  pkg:
    # Use of always() here ensures that even if the cross job is skipped we will still run
    if: ${{ always() && needs.prepare.outputs.package_build_rules != '{}' }}
    needs: prepare
    runs-on: ${{ fromJSON(needs.prepare.outputs.runs_on) }}
    # Build on the platform we are targeting in order to avoid https://github.com/rust-lang/rust/issues/57497.
    # Specifying container causes all of the steps in this job to run inside a Docker container (which is why the
    # cross-compilation needs to happen above in its own non-containerized job).
    container: ${{ matrix.image }}
    strategy:
      matrix: ${{ fromJSON(needs.prepare.outputs.package_build_rules) }}
    env:
      CARGO_DEB_VER: 1.42.2
      CARGO_GENERATE_RPM_VER: 0.16.1
      TOML_CLI_VER: 0.2.3
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3 # v4 doesn't work on containers with older GLIBC such as ubuntu:bionic or centos:7

    - name: Print matrix
      # Disable default use of bash -x for easier to read output in the log
      shell: bash
      run: |
        echo '${{ toJSON(matrix) }}'

    - name: Verify inputs
      id: verify
      run: |
        if [[ ! -f Cargo.toml ]]; then
          echo "::error::File 'Cargo.toml' is missing. This workflow is only intended for use with Rust Cargo projects."
          exit 1
        fi

        if [[ '${{ matrix.image }}' == '' ]]; then
          echo "::error::Required matrix variable 'image' is not defined in package_build_rules."
          exit 1
        fi

        if [[ '${{ matrix.target }}' == '' ]]; then
          echo "::error::Required matrix variable 'target' is not defined in package_build_rules."
          exit 1
        fi

        if [[ '${{ matrix.pkg }}' == '' ]]; then
          echo "pkg=${{ needs.prepare.outputs.cargo_name }}" >> $GITHUB_OUTPUT
        else
          echo "pkg=${{ matrix.pkg }}" >> $GITHUB_OUTPUT
        fi

    - name: Set vars
      id: setvars
      # Disable default use of bash -x for easier to read output in the log
      shell: bash
      env:
        MATRIX_IMAGE: ${{ matrix.image }}
        MATRIX_OS: ${{ matrix.os }}
      run: |
        # Get the operating system and release name (e.g. ubuntu and xenial) from the image name (e.g. ubuntu:xenial) by
        # extracting only the parts before and after but not including the colon:
        IMAGE="${MATRIX_IMAGE}"
        if [[ "${MATRIX_OS}" != "" ]]; then
          IMAGE="${MATRIX_OS}"
        fi

        if [[ "${IMAGE}" == "" ]]; then
          echo "::error::Matrix variable 'os' must be non-empty if set in package_build_rules."
          exit 1
        fi

        OS_NAME=${IMAGE%:*}
        OS_REL=${IMAGE#*:}

        if [[ "${OS_NAME}" == '' || "${OS_REL}" == '' ]]; then
          echo "::error::Matrix variable 'image' and/or 'os' must be of the form '<os name>:<os release>' in package_build_rules"
          exit 1
        fi

        case ${OS_NAME} in
          debian|ubuntu)
            ;;
          centos|rockylinux)
            ;;
          *)
            echo "::error::This workflow only supports 'debian', 'ubuntu', 'centos' or 'rockylinux' operating systems: '${IMAGE}' is not supported."
            exit 1
            ;;
        esac

        echo "OS_NAME=${IMAGE%:*}" >> $GITHUB_ENV
        echo "OS_REL=${IMAGE#*:}" >> $GITHUB_ENV

    # Allow CentOS 8 to continue working now that it is EOL
    # See: https://stackoverflow.com/a/70930049
    - name: CentOS 8 EOL workaround
      if: ${{ matrix.image == 'centos:8' }}
      run: |
        sed -i -e 's|mirrorlist=|#mirrorlist=|g' -e 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-Linux-*
    
    # See: https://github.com/NLnetLabs/ploutos
    # See: https://lists.debian.org/debian-devel-announce/2023/03/msg00006.html
    # See: https://unix.stackexchange.com/a/743865
    - name: Debian Stretch workaround
      if: ${{ matrix.image == 'debian:stretch' }}
      run: |
        echo "deb http://archive.debian.org/debian stretch main" > /etc/apt/sources.list
        echo "deb http://archive.debian.org/debian-security stretch/updates main" >> /etc/apt/sources.list

    # Install Rust the hard way rather than using a GH Action because the action doesn't work inside a Docker container.
    - name: Install Rust
      id: rust
      run: |
        case ${OS_NAME} in
          debian|ubuntu)
            apt-get update
            apt-get install -y curl
            ;;
        esac

        if ! hash cargo; then
          curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- --profile minimal -y
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH
          source "$HOME/.cargo/env"
          echo "bin_dir=$HOME/.cargo/bin" >> $GITHUB_OUTPUT
        else
          # Hack to determine the cargo install root directory so that we can tell the correct paths to actions/cache.
          # When using rustup to install Rust this is likely ~/.cargo/bin, but if using say the rust Docker image then
          # it is a different path entirely. The cargo command should already be installed so this should be almost a
          # no-op. Ideally it would be possible to just ask cargo but I haven't found a way. The docs list a 5-step
          # process for resolving the bin dir... (which involves checking --root, CARGO_INSTALL_ROOT, install.root,
          # CARGO_HOME and $HOME/.cargo...). Instead just find out where the cargo binary is installed and assume that
          # `cargo install` installed binaries will go there too.
          # See: https://doc.rust-lang.org/cargo/commands/cargo-install.html#description
          CARGO_BIN_DIR=$(dirname $(which cargo))
          echo "bin_dir=${CARGO_BIN_DIR}" >> $GITHUB_OUTPUT
        fi

    - name: Install compilation and other dependencies
      run: |
        case ${OS_NAME} in
          debian|ubuntu)
            apt-get install -y binutils gcc dpkg-dev jq lintian pkg-config ${{ inputs.deb_extra_build_packages }}
            ;;
          centos|rockylinux)
            #sed -i -e 's/enabled=1/enabled=0/' /etc/yum/pluginconf.d/fastestmirror.conf || true
            yum update -y # See: https://github.com/NLnetLabs/ploutos/issues/90
            yum install epel-release -y
            yum install -y findutils gcc jq ${{ inputs.rpm_extra_build_packages }}

            # note: we also install python magic otherwise binaries are not identified as such and rpmlint outputs:
            #   E: no-binary
            case ${OS_REL} in
              7)
                yum install -y python-magic rpmlint
                ;;

              *)
                # assume we are only invoked with something newer than CentOS 7, e.g. not CentOS 6 ;-)
                yum install -y cpio python3-pip
                pip3 install --upgrade pip
                pip3 install python-magic rpmlint
                ;;
            esac
            ;;
        esac

    # Speed up Rust builds by caching unchanged built dependencies.
    # See: https://github.com/actions/cache/blob/master/examples.md#rust---cargo
    - name: Fetch .cargo from cache
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
        key: ${{ matrix.image }}-${{ steps.verify.outputs.pkg }}-cargo-${{ hashFiles('**/Cargo.lock') }}

    # Speed up tooling installation by only re-downloading and re-building dependent crates if we change the version of
    # the tool that we are using.
    - name: Fetch cargo-deb from cache
      id: cache-cargo-deb
      uses: actions/cache@v3
      with:
        path: ${{ steps.rust.outputs.bin_dir }}/cargo-deb
        key: ${{ matrix.image }}-cargo-deb-${{ env.CARGO_DEB_VER }}-${{ endsWith(matrix.image, 'xenial')}}

    - name: Fetch cargo-generate-rpm from cache
      id: cache-cargo-generate-rpm
      uses: actions/cache@v3
      with:
        path: ${{ steps.rust.outputs.bin_dir }}/cargo-generate-rpm
        key: ${{ matrix.image }}-cargo-generate-rpm-${{ env.CARGO_GENERATE_RPM_VER }}

    - name: Fetch toml-cli from cache
      id: cache-toml-cli
      uses: actions/cache@v3
      with:
        path: ${{ steps.rust.outputs.bin_dir }}/toml
        key: ${{ matrix.image }}-toml-cli-${{ env.TOML_CLI_VER }}

    # Only install cargo-deb or cargo-generate-rpm if not already fetched from the cache.
    - name: Install cargo-deb if needed
      id: install-cargo-deb
      if: ${{ steps.cache-cargo-deb.outputs.cache-hit != 'true' }}
      run: |
        case ${OS_NAME} in
          debian|ubuntu)
            if [[ "${OS_REL}" == "xenial" ]]; then
              # Disable use of the default lzma feature which causes XZ compression to be used which then causes Lintian
              # to fail with error:
              #   E: krill: malformed-deb-archive newer compressed control.tar.xz
              # Passing --fast to cargo-deb to disable use of XZ compression didn't help.
              # See: https://github.com/kornelski/cargo-deb/issues/12
              EXTRA_CARGO_INSTALL_ARGS="--no-default-features"
            else
              EXTRA_CARGO_INSTALL_ARGS=""
            fi
            cargo install cargo-deb --version ${CARGO_DEB_VER} ${EXTRA_CARGO_INSTALL_ARGS}
            echo "installed=true" >> $GITHUB_OUTPUT
            ;;
        esac

    - name: Force cache save
      if: ${{ steps.install-cargo-deb.outputs.installed == 'true' }}
      uses: actions/cache/save@v3
      with:
        path: ${{ steps.rust.outputs.bin_dir }}/cargo-deb
        key: ${{ matrix.image }}-cargo-deb-${{ env.CARGO_DEB_VER }}-${{ endsWith(matrix.image, 'xenial')}}

    - name: Install cargo-generate-rpm if needed
      id: install-cargo-generate-rpm
      if: ${{ steps.cache-cargo-generate-rpm.outputs.cache-hit != 'true' }}
      run: |
        case ${OS_NAME} in
          centos|rockylinux)
            cargo install cargo-generate-rpm --version ${CARGO_GENERATE_RPM_VER}
            echo "installed=true" >> $GITHUB_OUTPUT
            ;;
        esac

    - name: Force cache save
      if: ${{ steps.install-cargo-generate-rpm.outputs.installed == 'true' }}
      uses: actions/cache/save@v3
      with:
        path: ${{ steps.rust.outputs.bin_dir }}/cargo-generate-rpm
        key: ${{ matrix.image }}-cargo-generate-rpm-${{ env.CARGO_GENERATE_RPM_VER }}

    - name: Install toml-cli if needed
      id: install-toml-cli
      if: ${{ steps.cache-toml-cli.outputs.cache-hit != 'true' }}
      run: |
        cargo install toml-cli --version ${TOML_CLI_VER}
        echo "installed=true" >> $GITHUB_OUTPUT

    - name: Force cache save
      if: ${{ steps.install-toml-cli.outputs.installed == 'true' }}
      uses: actions/cache/save@v3
      with:
        path: ${{ steps.rust.outputs.bin_dir }}/toml
        key: ${{ matrix.image }}-toml-cli-${{ env.TOML_CLI_VER }}

    - name: Download cross-compiled binaries
      if: ${{ matrix.target != 'x86_64' }}
      uses: NLnetLabs/download-artifact@main
      with:
        name: ${{ inputs.artifact_prefix }}tmp-cross-binaries-${{ matrix.target }}
        path: .
        maxTries: ${{ needs.prepare.outputs.cross_max_tries }}
        retryDelayMs: ${{ needs.prepare.outputs.cross_retry_delay_ms }}
  
    - name: Untar the set of downloaded binaries
      if: ${{ matrix.target != 'x86_64' }}
      run: tar vpxf bins.tar

    # Instruct cargo-deb or cargo-generate-rpm to build the package based on Cargo.toml settings and command line
    # arguments.
    - name: Create the package
      id: create
      env:
        MATRIX_PKG: ${{ steps.verify.outputs.pkg }}
        EXTRA_BUILD_ARGS: ${{ matrix.extra_build_args }}
        EXTRA_CARGO_DEB_ARGS: ${{ matrix.extra_cargo_deb_args }}
        CROSS_TARGET: ${{ matrix.target }}
      run: |
        # Debian
        # ==============================================================================================================
        # Packages for different distributions (e.g. Stretch, Buster) of the same O/S (e.g. Debian) when served from a
        # single package repository MUST have unique package_ver_architecture triples. Cargo deb can vary the name based
        # on the 'variant' config section in use, but doesn't do so according to Debian policy (as it modifies the
        # package name, not the package version).
        #   Format: package_ver_architecture
        #   Where ver has format: [epoch:]upstream_version[-debian_revision]
        #   And debian_version should be of the form: 1<xxx>
        #   Where it is common to set <xxx> to the O/S name.
        # See:
        #   - https://unix.stackexchange.com/a/190899
        #   - https://www.debian.org/doc/debian-policy/ch-controlfields.html#version
        # Therefore we generate the version ourselves.
        #
        # In addition, Semantic Versioning and Debian version policy cannot express a pre-release label in the same way.
        # For example 0.8.0-rc.1 is a valid Cargo.toml [package].version value but when used as a Debian package version
        # 0.8.0-rc.1 would be considered _NEWER_ than the final 0.8.0 release. To express this in a Debian compatible
        # way we must replace the dash '-' with a tilda '~'.
        #
        # RPM
        # ==============================================================================================================
        # Handle the release candidate case where the version string needs to have dash replaced by tilda. The cargo
        # build command won't work if the version key in Cargo.toml contains a tilda but we have to put the tilda there
        # for when we run cargo generate-rpm so that it uses it.
        # 
        # For background on RPM versioning see:
        #   https://docs.fedoraproject.org/en-US/packaging-guidelines/Versioning/
        #
        # COMMON
        # ==============================================================================================================
        # Finally, sometimes we want a version to be NEWER than the latest release but without having to decide what
        # higher semver number to bump to. In this case we do NOT want dash '-' to become '~' because `-` is treated as
        # higher and tilda is treated as lower.

        if [[ '${{ matrix.image }}' == '' ]]; then
          echo "::error::Required matrix variable 'image' is not defined in package_build_rules."
          exit 1
        fi

        APP_VER=$(cargo read-manifest ${{ needs.prepare.outputs.cargo_read_manifest_path_arg }} | jq -r '.version')
        APP_NEW_VER=$(echo $APP_VER | tr '-' '~')
        NEXT_VER_LABEL="${{ inputs.next_ver_label }}"
        PKG_APP_VER=$(echo $APP_NEW_VER | sed -e "s/~$NEXT_VER_LABEL/-$NEXT_VER_LABEL/")

        TARGET_DIR="target"
        if [[ '${{ inputs.manifest_dir }}' != '' ]]; then
          TARGET_DIR="${{ inputs.manifest_dir }}/target"
        fi
        echo "target_dir=${TARGET_DIR}" >> $GITHUB_OUTPUT

        case ${OS_NAME} in
          debian|ubuntu)
            # Ugly hack to use an alternate base Cargo Deb configuration so that the selected variant overrides settings
            # in the specified alternate base settings instead of the usual [package.metadata.deb] base settings.
            if grep -Eq "^\[package\.metadata\.deb_alt_base_${MATRIX_PKG}\]$" ${{ needs.prepare.outputs.cargo_package_toml_path }}; then
              sed -i -e "s/^\[package\.metadata\.deb\]/[package.metadata.moved-deb]/" \
                     -e "s/^\[package\.metadata\.deb_alt_base_${MATRIX_PKG}\]/[package.metadata.deb]/" \
                     ${{ needs.prepare.outputs.cargo_package_toml_path }}
            fi

            VARIANT="${OS_NAME}-${OS_REL}"

            # Older O/S releases come with an older version of systemd which understands far fewer keys in the systemd
            # unit file. As such if we detect that we are installing to an older O/S we will use a "minimal" cargo-deb
            # profile, if one is defined in `Cargo.toml`.
            MINIMAL_VARIANT="minimal"

            # When cross-compiling we don't use cargo-deb to do compilation, only packaging, which means that cargo-deb is
            # not able to automatically determine install-time package dependencies for us (which happens if `depends` in
            # `Cargo.toml` is either empty or contains '$auto'). Instead the set of dependent packages must be specified
            # manually. The project being packaged can either define a cross-target specific `cargo-deb` profile, or a 
            # "minimal" profile suitable for cross-compiled targets.
            if [[ "${CROSS_TARGET}" != "x86_64" ]]; then
              EXTRA_CARGO_DEB_ARGS="--no-build --no-strip --target ${CROSS_TARGET} --output ${TARGET_DIR}/debian ${EXTRA_CARGO_DEB_ARGS}"
              MINIMAL_VARIANT="minimal-cross"
              VARIANT="${OS_NAME}-${OS_REL}-${CROSS_TARGET}"
            fi

            # Prefer the "minimal" cargo-deb profile, if one exists in `Cargo.toml`:
            if grep -qF "[package.metadata.deb.variants.${MINIMAL_VARIANT}]" ${{ needs.prepare.outputs.cargo_package_toml_path }}; then
                case ${OS_REL} in
                  xenial|bionic|stretch)
                    VARIANT="${MINIMAL_VARIANT}"
                    ;;
                esac
            fi

            OPT_VARIANT_ARG=""
            if [[ "${VARIANT}" != "" ]]; then
              if grep -qF "[package.metadata.deb.variants.${VARIANT}]" ${{ needs.prepare.outputs.cargo_package_toml_path }}; then
                OPT_VARIANT_ARG="--variant ${VARIANT}"
              else
                echo "::notice file=Cargo.toml::Cargo deb variant '${VARIANT}' not found, using defaults instead."
              fi
            fi

            CHANGELOG_KEY="package.metadata.deb.changelog"
            GEN_CHANGELOG_NAME="debian/changelog"
            GEN_CHANGELOG_PATH="${TARGET_DIR}/${GEN_CHANGELOG_NAME}"
            SET_CHANGELOG_PATH=$(toml get ${{ needs.prepare.outputs.cargo_package_toml_path }} ${CHANGELOG_KEY} || echo null)

            case ${SET_CHANGELOG_PATH} in
              null|'"'${GEN_CHANGELOG_PATH}'"')
                #
                # Generate the changelog file that Debian packages are required to have.
                # See: https://www.debian.org/doc/manuals/maint-guide/dreq.en.html#changelog
                #

                # 1. Use the same logic as cargo-deb, i.e. take the maintainer from the package.metadata.deb.maintainer
                #    key in Cargo.toml (for the selected variant) if defined, else use the first author instead.
                #
                # Note:
                #    With jq 1.6: del(..|nulls)
                #    With jq 1.5: del(recurse(.[]?;true)|select(. == null))
                #    Older systems only have jq 1.5 so we have to use the more verbose construct.
                V=${VARIANT:-null}
                MAINTAINER=$(cargo read-manifest ${{needs.prepare.outputs.cargo_read_manifest_path_arg}} | jq -r '[.metadata.deb.variants."'$V'".maintainer, .metadata.deb.maintainer, .authors[0]] | del(recurse(.[]?;true)|select(. == null))[0]')

                # 2. Generate the RFC 5322 format date by hand instead of using date --rfc-email because that option doesn't
                #    exist on Ubuntu 16.04 and Debian 9
                RFC5322_TS=$(LC_TIME=en_US.UTF-8 date +'%a, %d %b %Y %H:%M:%S %z')

                # 3. Generate the changelog file that Debian packages are required to have.
                if [ ! -d ${TARGET_DIR}/debian ]; then
                  mkdir -p ${TARGET_DIR}/debian
                fi
                echo "${MATRIX_PKG} (${PKG_APP_VER}) unstable; urgency=medium" > ${GEN_CHANGELOG_PATH}
                echo "  * See: https://github.com/${{ github.repository }}/releases/tag/v${APP_NEW_VER}" >> ${GEN_CHANGELOG_PATH}
                echo " -- maintainer ${MAINTAINER}  ${RFC5322_TS}" >> ${GEN_CHANGELOG_PATH}

                # 4. Print the generated changelog for diagnostic purposes
                echo "Generated changelog:"
                cat ${GEN_CHANGELOG_PATH}

                # 5. Configure cargo-deb to use the generated changelog file
                mv ${{ needs.prepare.outputs.cargo_package_toml_path }} ${{ needs.prepare.outputs.cargo_package_toml_path }}.org
                toml set \
                  ${{ needs.prepare.outputs.cargo_package_toml_path }}.org \
                  package.metadata.deb.changelog $(readlink -f ${GEN_CHANGELOG_PATH}) > \
                  ${{ needs.prepare.outputs.cargo_package_toml_path }}

                #
                # End changelog generation
                #
                ;;
            esac

            DEB_VER="${PKG_APP_VER}-1${OS_REL}"

            # This shouldn't be necessary...
            rm -f ${TARGET_DIR}/debian/*.deb

            cargo deb \
              ${{needs.prepare.outputs.cargo_manifest_path_arg}} \
              ${{needs.prepare.outputs.cargo_workspace_package_arg}} \
              --deb-version ${DEB_VER} \
              ${OPT_VARIANT_ARG} \
              -v ${EXTRA_CARGO_DEB_ARGS} \
              -- \
              --locked \
              ${EXTRA_BUILD_ARGS}
            ;;

          centos|rockylinux)
            # Build and strip our app binaries as cargo generate-rpm doesn't do this for us
            cargo build \
              ${{needs.prepare.outputs.cargo_manifest_path_arg}} \
              ${{needs.prepare.outputs.cargo_workspace_package_arg}} \
              --release \
              --locked \
              -v \
              ${EXTRA_BUILD_ARGS}

            # TODO: It might be possible to replace the hacky copying of the service file below with some clever use of
            # `--set-metadata` when invoking cargo generate-rpm. Of particular interest is the new `--variant` command
            # line argument which might enable us to work the same way as we do for cargo deb above.
            # See: https://github.com/cat-in-136/cargo-generate-rpm/issues/18

            ## Determine any additional arguments that need to be passed to cargo generate-rpm
            #case "${OS_NAME}:${OS_REL}" in
            #  centos:7)
            #    # yum install fails on older CentOS with the default LZMA compression used by cargo generate-rpm since v0.5.0
            #    # see: https://github.com/cat-in-136/cargo-generate-rpm/issues/30
            #    EXTRA_CARGO_GENERATE_RPM_ARGS="--payload-compress gzip"
            #    ;;
            #  *)
            #    # assume we are only invoked with something newer than CentOS 7, e.g. not CentOS 6 ;-)
            #    EXTRA_CARGO_GENERATE_RPM_ARGS=""
            #    ;;
            #esac

            # Hack to use a different service file without having to duplicate almost the entire 
            # [package.metadata.generate-rpm.assets] setting with only one entry changed. We don't need this with
            # cargo-deb because it handles systemd service file selection automatically based on factors like the
            # current variant in use.

            # Support the older matrix variable name for backward compatibility.
            if [[ "${{ matrix.rpm_systemd_service_unit_file }}" != "" ]]; then
                SYSTEMD_SERVICE_UNIT_FILE="${{ matrix.rpm_systemd_service_unit_file }}"
            else
                SYSTEMD_SERVICE_UNIT_FILE="${{ matrix.systemd_service_unit_file }}"
            fi

            if [ -e "${SYSTEMD_SERVICE_UNIT_FILE}" ]; then
                mkdir -p ${TARGET_DIR}/rpm/
                cp ${SYSTEMD_SERVICE_UNIT_FILE} ${TARGET_DIR}/rpm/${MATRIX_PKG}.service
            elif [[ "${SYSTEMD_SERVICE_UNIT_FILE}" == *"*" ]]; then
                mkdir -p ${TARGET_DIR}/rpm/
                cp ${SYSTEMD_SERVICE_UNIT_FILE} ${TARGET_DIR}/rpm
            fi

            # Ugly hack to use an alternate base Cargo Generate RPM configuration so that the selected variant overrides
            # settings specified alternate base settings instead of the usual [package.metadata.generate-rpm] base
            # settings.
            RPM_SCRIPTLETS_PATH="${{ inputs.rpm_scriptlets_path }}"
            if grep -Eq "^\[package\.metadata\.generate-rpm-alt-base-${MATRIX_PKG}\]$" ${{ needs.prepare.outputs.cargo_package_toml_path }}; then
              if [[ "${RPM_SCRIPTLETS_PATH}" != "" ]]; then
                RPM_SCRIPTLETS_PATH="${RPM_SCRIPTLETS_PATH}-${MATRIX_PKG}"
              fi
              sed -i -e "s/^\[package\.metadata\.generate-rpm\]/[package.metadata.moved-generate-rpm]/" \
                     -e "s/^\[package\.metadata\.generate-rpm-alt-base-${MATRIX_PKG}\]/[package.metadata.generate-rpm]/" \
                     ${{ needs.prepare.outputs.cargo_package_toml_path }}
            fi

            # https://github.com/NLnetLabs/krill/issues/907
            # cargo-generate-rpm doesn't support setting scripts to files, and we can't refer to a file that 
            # installed such as /usr/share/krill/rpm/postuninst because 'yum remove' removes the file before it
            # can be executed (hence post-uninstall rather than pre-uninstall...). So instead embed the entire
            # uninstall script in the TOML settings using the cargo-generate-rpm --set-metadata (aka -s)
            # command line argument.
            if [ -f "${RPM_SCRIPTLETS_PATH}" ]; then
              # Download the RPM systemd macros shell functions file fragment
              SYSTEMD_RPM_MACROS_URL="https://raw.githubusercontent.com/NLnetLabs/ploutos/v7.0.2/fragments/macros.systemd.sh"
              SYSTEMD_RPM_MACROS_FILE="/tmp/systemd_rpm_macros"
              curl --proto '=https' --tlsv1.2 --fail --output ${SYSTEMD_RPM_MACROS_FILE} ${SYSTEMD_RPM_MACROS_URL}

              # Replace any reference to #SYSTEMD_RPM_MACROS# in the scriptlet file with the downloaded RPM systemd macros
              # shell fragment.
              SCRIPTLET_FILE="${{ inputs.rpm_scriptlets_path }}"
              mv "${SCRIPTLET_FILE}" "${SCRIPTLET_FILE}.org"
              templ=$(<${SCRIPTLET_FILE}.org)
              value=$(<$SYSTEMD_RPM_MACROS_FILE)
              echo "${templ//#RPM_SYSTEMD_MACROS#/$value}" > "$SCRIPTLET_FILE"

              # Configure ourselves to use the updated scriptlets file as input to cargo generate-rpm.
              SCRIPTLETS='--metadata-overwrite=${{ inputs.rpm_scriptlets_path }}'
            fi

            find ${TARGET_DIR}/release -maxdepth 1 -type f -executable | xargs strip -s -v

            # This shouldn't be necessary...
            rm -f ${TARGET_DIR}/generate-rpm/*.rpm

            if [[ '${{inputs.manifest_dir}}' != '' ]]; then
              pushd ${{inputs.manifest_dir}}
            fi
  
            cargo generate-rpm ${{needs.prepare.outputs.cargo_workspace_package_arg}} \
                --set-metadata "version=\"${PKG_APP_VER}\"" \
                ${SCRIPTLETS} \
                ${EXTRA_CARGO_GENERATE_RPM_ARGS}

            if [[ '${{inputs.manifest_dir}}' != '' ]]; then
              popd
            fi

            ;;
        esac

    - name: Post-process the package
      run: |
        TARGET_DIR="${{ steps.create.outputs.target_dir}}"

        case ${OS_NAME} in
          debian|ubuntu)
            # https://github.com/NLnetLabs/routinator/issues/783
            # Patch the generated DEB to have ./ paths compatible with `unattended-upgrade`:
            ls -la ${TARGET_DIR}/debian/

            pushd ${TARGET_DIR}/debian
            DEB_FILE_NAME=$(ls -1 *.deb | head -n 1)
            DATA_ARCHIVE=$(ar t ${DEB_FILE_NAME} | grep -E '^data\.tar')
            ar x ${DEB_FILE_NAME} ${DATA_ARCHIVE}
            tar tf ${DATA_ARCHIVE}
            EXTRA_TAR_ARGS=
            if [[ "${DATA_ARCHIVE}" == *.xz ]]; then
              # Install XZ support that will be needed by TAR
              apt-get -y install -y xz-utils
              EXTRA_TAR_ARGS=J
            fi
            mkdir tar-hack
            tar -C tar-hack -xf ${DATA_ARCHIVE}
            pushd tar-hack
            tar c${EXTRA_TAR_ARGS}f ../${DATA_ARCHIVE} ./*
            popd
            tar tf ${DATA_ARCHIVE}
            ar r ${DEB_FILE_NAME} ${DATA_ARCHIVE}
            popd

            ls -la ${TARGET_DIR}/debian/

            pushd ${TARGET_DIR}/debian
            dpkg -e ${DEB_FILE_NAME} control_files
            dpkg -x ${DEB_FILE_NAME} data_files

            for D in control_files data_files; do
              pushd $D

              echo Listing package $D and sizes
              find . -type f -exec du -sh {} \;

              echo
              echo Printing non-binary files contained within $D

              # Use grep to exclude find only non-binary files that we can print
              find . -type f -exec grep -Il '.' {} \; -exec cat {} \; -exec echo \; -exec echo \;

              popd
              rm -R $D
            done
            popd
            ;;

          centos|rockylinux)
            ls -la ${TARGET_DIR}/generate-rpm/

            pushd ${TARGET_DIR}/generate-rpm
            for F in *.rpm; do
              echo Printing package info
              rpm -qip $F
  
              echo $F
              mkdir t
              pushd t
              rpm2cpio ../$F | cpio -idmv || true

              echo Listing package files and sizes
              find . -type f -exec du -sh {} \;

              echo
              echo Printing non-binary files contained within the package

              # Use grep to exclude find only non-binary files that we can print
              find . -type f -exec grep -Il '.' {} \; -exec cat {} \; -exec echo \; -exec echo \;

              popd
              rm -R t

              echo Printing package scriptlets
              rpm -qp --scripts $F
            done

            popd
            ;;
        esac

    # See what O/S specific linting tools think of our package.
    - name: Verify the package
      env:
        CROSS_TARGET: ${{ matrix.target }}
      run: |
        TARGET_DIR="${{ steps.create.outputs.target_dir}}"
        if [[ '${{ inputs.manifest_dir }}' != '' ]]; then
          TARGET_DIR="${{ inputs.manifest_dir }}/target"
        fi

        case ${OS_NAME} in
          debian|ubuntu)
            dpkg --info ${TARGET_DIR}/debian/*.deb

            EXTRA_LINTIAN_ARGS="${{ matrix.deb_extra_lintian_args }}"
            EXTRA_LINTIAN_ARGS="${EXTRA_LINTIAN_ARGS} --suppress-tags manpage-not-compressed-with-max-compression"

            case ${OS_REL} in
              xenial|bionic|focal|stretch|buster)
                ;;

              *)
                EXTRA_LINTIAN_ARGS="${EXTRA_LINTIAN_ARGS} --suppress-tags poor-compression-in-manual-page"
                ;;
            esac

            if [[ "${CROSS_TARGET}" != "x86_64" ]]; then
              EXTRA_LINTIAN_ARGS="${EXTRA_LINTIAN_ARGS} --suppress-tags unstripped-binary-or-object,statically-linked-binary"
            fi

            if [[ "${{ inputs.strict_mode }}" == "true" ]]; then
              case ${OS_REL} in
                focal)
                  ;;

                xenial|bionic|stretch|buster)
                  EXTRA_LINTIAN_ARGS="${EXTRA_LINTIAN_ARGS} --fail-on-warnings"
                  ;;

                *)
                  EXTRA_LINTIAN_ARGS="${EXTRA_LINTIAN_ARGS} --fail-on error,warning"
                  ;;
              esac
            else
              case ${OS_REL} in
                xenial|bionic|focal|stretch|buster)
                  ;;

                *)
                  EXTRA_LINTIAN_ARGS="${EXTRA_LINTIAN_ARGS} --fail-on error"
                  ;;
              esac
            fi

            lintian --version
            lintian --allow-root -v ${EXTRA_LINTIAN_ARGS} ${TARGET_DIR}/debian/*.deb
            ;;

          centos|rockylinux)
            # cargo generate-rpm creates RPMs that rpmlint considers to have errors so don't use the rpmlint exit code
            # otherwise we will always abort the workflow.

            # rpmlint checks are failing that relate to "tags" that cargo-generate-rpm does not currently support
            # setting. The only way to disable certain checks is by writing a config file to disk. At the time of
            # writing versions of rpmlint encountered were:
            #
            #    centos:7 Docker image    : rpmlint 1.5
            #    rockylinux:8 Docker image: rpmlint 1.10
            #    rockylinux:8 Docker image: rpmlint 2.4.0 (via pip3 install rpmlint)
            #    fedora:37 Docker image:    rpmlint 2.2.0
            #
            # CentOS / RockyLinux 8 supports rpmlint 2.x via python3-pip, so we can use the newer version instead, but
            # this can't be installed on CentOS 7 due to a missing rpm-python3 RPM bindings package needed by pip3
            # install.
            #
            # See: https://github.com/rpm-software-management/rpmlint/blob/2.4.0/README.md#configuration
            # See: https://github.com/rpm-software-management/rpmlint/blob/rpmlint-1.4/README#L39
            # See: https://github.com/cat-in-136/cargo-generate-rpm/issues/20

            EXTRA_RPMLINT_ARGS=
            LINTER_CONFIG_PATH="$HOME/.config/rpmlint"

            #case ${OS_REL} in
            #  7)
            #    # On CentOS 7 we only have the older rpmlint 1.x available, which has a known issue with the
            #    # `missing-call-to-chdir-with-chroot` check causing false positives, therefore we filter this error
            #    # out. The false positive issue was fixed in rpmlint 2.x.
            #    #
            #    # See: https://github.com/rpm-software-management/rpmlint/issues/84
            #    mkdir $HOME/.config
            #    cat <<'EOF' >${LINTER_CONFIG_PATH}
            #    from Config import *
            #    addFilter(".: no-buildhost-tag")
            #    addFilter(".: no-changelogname-tag")
            #    addFilter(".: missing-call-to-chdir-with-chroot")
            #    EOF
            #    ;;

            #  *)
                # Assume we are only invoked with something newer than CentOS 7, e.g. not CentOS 6 ;-)
                if [[ "${{ inputs.strict_mode }}" == "true" ]]; then
                  EXTRA_RPMLINT_ARGS="${EXTRA_RPMLINT_ARGS} --strict"
                fi

                mkdir $HOME/.config
                cat <<'EOF' >${LINTER_CONFIG_PATH}
        addFilter(".: no-buildhost-tag")
        addFilter(".: no-changelogname-tag")
        addFilter(".: no-group-tag")
        addFilter(".: no-packager-tag")
        addFilter(".: no-signature")
        EOF

                # rpmlint 2.x requires that we explicitly tell it where the config file is, and the pip installed
                # version doesn't come with any existing list of valid licenses so we have to install one ourselves.
                # With rpmlint 1.x, or rpmlint 2.x installed from O/S package (e.g. on fedora:37) the valid license
                # set is pre-loaded, but that doesn't help us here. If we don't solve this a warning, or error in
                # strict mode, will be raised about the unknown license used by the RPM being checked (as all
                # licenses are unknown!).
                LICENSES_CONF_URL="https://raw.githubusercontent.com/rpm-software-management/rpmlint/2.4.0/configs/Fedora/licenses.toml"
                LICENSES_CONF_PATH="$HOME/.config/rpmlint-licenses.conf"
                curl --proto '=https' --tlsv1.2 --fail --output ${LICENSES_CONF_PATH} ${LICENSES_CONF_URL}

                EXTRA_RPMLINT_ARGS="${EXTRA_RPMLINT_ARGS} --rpmlintrc ${LINTER_CONFIG_PATH} --config ${LICENSES_CONF_PATH}"
            #    ;;
            #esac

            for CHECK_NAME in ${{ matrix.rpm_rpmlint_check_filters }}; do
              echo 'addFilter(".: '${CHECK_NAME}'")' >> ${LINTER_CONFIG_PATH}
            done

            cat ${LINTER_CONFIG_PATH}

            rpmlint --version
            rpmlint ${EXTRA_RPMLINT_ARGS} ${TARGET_DIR}/generate-rpm/*.rpm
            ;;
        esac

    # Upload the produced package. The artifact will be available via the GH Actions job summary and build log pages,
    # but only to users logged in to GH with sufficient rights in this project. The uploaded artifact is also downloaded
    # by the next job (see below) to sanity check that it can be installed and results in a working Krill installation.
    - name: Upload package
      uses: NLnetLabs/upload-artifact@main
      with:
        name: ${{ inputs.artifact_prefix }}${{ steps.verify.outputs.pkg }}_${{ env.OS_NAME }}_${{ env.OS_REL }}_${{ matrix.target }}
        path: |
          ${{ steps.create.outputs.target_dir }}/debian/*.deb
          ${{ steps.create.outputs.target_dir }}/generate-rpm/*.rpm
        if-no-files-found: warn

  # -------------------------------------------------------------------------------------------------------------------
  # Job: 'pkg-test'
  # -------------------------------------------------------------------------------------------------------------------
  # Download and sanity check on target operating systems the packages created by previous jobs (see above). Don't test
  # on GH runners as they come with lots of software and libraries pre-installed and thus are not representative of the
  # actual deployment targets, nor do GH runners support all targets that we want to test. Don't test in Docker
  # containers as they do not support systemd.
  pkg-test:
    # Use of always() here ensures that even if the _cross_ job (note: not the 'pkg' job) is skipped we will still run.
    # I wouldn't expect this to be needed but since the cross job was made conditional we seem to need this.
    if: ${{ always() && needs.prepare.outputs.package_test_rules != '{}' }}
    needs: [pkg, prepare]
    # If runs_on is not overridden by the user, and we are about to test a package in a CentOS 7 LXC container, force
    # the host O/S to be the older Ubuntu 20.04 as Ubuntu 22.04 upgraded from CGroupV1 to CGroupV2 which is incompatible
    # with the CentOS 7 LXC image. Use a sort-of ternary if syntax that GitHub Actions supports in expressions to effect
    # this dynamically per matrix invocation of the `pkg-test` job.
    # See:
    #   - https://github.com/NLnetLabs/ploutos/issues/50
    #   - https://github.com/actions/runner/issues/409#issuecomment-727565588
    runs-on: ${{ (inputs.runs_on == 'ubuntu-22.04' && (matrix.image == 'centos:7' || matrix.image == 'ubuntu:xenial')) && 'ubuntu-20.04' || inputs.runs_on }}
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.prepare.outputs.package_test_rules) }}
    steps:
    # Fetch the test scripts that we will run
    - name: Checkout repository
      uses: actions/checkout@v3 # v4 doesn't work on containers with older GLIBC such as ubuntu:bionic or centos:7

    - name: Print matrix
      # Disable default use of bash -x for easier to read output in the log
      shell: bash
      run: |
        echo '${{ toJSON(matrix) }}'

    - name: Verify inputs
      id: verify
      run: |
        if [[ '${{ matrix.image }}' == '' ]]; then
          echo "::error::Required matrix variable 'image' is not defined in package_test_rules."
          exit 1
        fi

        if [[ '${{ matrix.target }}' == '' ]]; then
          echo "::error::Required matrix variable 'target' is not defined in package_test_rules."
          exit 1
        fi

        if [[ '${{ matrix.pkg }}' == '' ]]; then
          echo "pkg=${{ needs.prepare.outputs.cargo_name }}" >> $GITHUB_OUTPUT
        else
          echo "pkg=${{ matrix.pkg }}" >> $GITHUB_OUTPUT
        fi

        if [[ '${{ matrix.published_pkg }}' == '' ]]; then
          PUBLISHED_PKG="$PKG"
        else
          PUBLISHED_PKG="${{ matrix.published_pkg }}"
        fi
        echo "published_pkg=$PUBLISHED_PKG" >> $GITHUB_OUTPUT

        if [[ '${{ matrix.mode }}' == '' ]]; then
          echo "mode=fresh-install" >> $GITHUB_OUTPUT
        else
          echo "mode=${{ matrix.mode }}" >> $GITHUB_OUTPUT
        fi

        if [[ '${{ matrix.os }}' == '' ]]; then
          echo "image=${{ matrix.image }}" >> $GITHUB_OUTPUT
        else
          echo "image=${{ matrix.os }}" >> $GITHUB_OUTPUT
        fi

        if [[ '${{ matrix.deb_apt_key_url }}' == '' ]]; then
          DEB_APT_KEY_URL='https://packages.nlnetlabs.nl/aptkey.asc'
        else
          DEB_APT_KEY_URL="${{ matrix.deb_apt_key_url }}"
        fi
        echo "deb_apt_key_url=$DEB_APT_KEY_URL" >> $GITHUB_OUTPUT

        if [[ '${{ matrix.deb_apt_source }}' == '' ]]; then
          DEB_APT_SOURCE='deb [arch=amd64] https://packages.nlnetlabs.nl/linux/${OS_NAME}/ ${OS_REL} main'
        else
          DEB_APT_SOURCE="${{ matrix.deb_apt_source }}"
        fi
        echo "deb_apt_source=$DEB_APT_SOURCE" >> $GITHUB_OUTPUT

        if [[ '${{ matrix.rpm_yum_key_url }}' == '' ]]; then
          RPM_YUM_KEY_URL="https://packages.nlnetlabs.nl/aptkey.asc"
        else
          RPM_YUM_KEY_URL="${{ matrix.rpm_yum_key_url }}"
        fi
        echo "rpm_yum_key_url=$RPM_YUM_KEY_URL" >> $GITHUB_OUTPUT

        if [[ '${{ matrix.rpm_yum_repo }}' == '' ]]; then
          RPM_YUM_REPO='[nlnetlabs]\nname=NLnet Labs\nbaseurl=https://packages.nlnetlabs.nl/linux/centos/$releasever/main/$basearch\nenabled=1'
        else
          RPM_YUM_REPO="${{ matrix.rpm_yum_repo }}"
        fi
        {
          echo "rpm_yum_repo<<EOF'
          echo $RPM_YUM_REPO"
          echo 'EOF'
        } >> $GITHUB_OUTPUT

    # Set some environment variables that will be available to "run" steps below in this job, and some output variables
    # that will be available in GH Action step definitions below.
    - name: Set vars
      id: setvars
      # Disable default use of bash -x for easier to read output in the log
      shell: bash
      env:
        MATRIX_IMAGE: ${{ steps.verify.outputs.image }}
      run: |
        # Get the operating system and release name (e.g. ubuntu and xenial) from the image name (e.g. ubuntu:xenial) by
        # extracting only the parts before and after but not including the colon:
        OS_NAME=${MATRIX_IMAGE%:*}
        OS_REL=${MATRIX_IMAGE#*:}

        if [[ "${OS_NAME}" == '' || "${OS_REL}" == '' ]]; then
          echo "::error::Matrix variable 'image' must be of the form '<os name>:<os release>' in package_test_rules"
          exit 1
        fi

        echo "OS_NAME=${OS_NAME}" >> $GITHUB_ENV
        echo "OS_REL=${OS_REL}" >> $GITHUB_ENV

        if [[ '${{ matrix.test-image }}' != '' ]]; then
          # convert os_name:os_rel to os_name/os_rel. The colon form is consistent with the values the user must
          # otherwise provide as input, e.g. via the 'image' matrix entry, but LXC images separate the image name
          # from the image release using a forward slash rather than a colon so we have to convert.
          TEST_IMAGE="${{ matrix.test-image }}"
          LXC_IMAGE_NAME=${TEST_IMAGE%:*}
          LXC_IMAGE_REL=${TEST_IMAGE#*:}
          echo "LXC_IMAGE=images:${LXC_IMAGE_NAME}/${LXC_IMAGE_REL}/cloud" >> $GITHUB_ENV
        else
          case ${MATRIX_IMAGE} in
            centos:8)
              # the CentOS 8 LXD image no longer exists since CentOS 8 hit EOL.
              # use the Rocky Linux (a CentOS 8 compatible O/S) LXD image instead.
              echo "LXC_IMAGE=images:rockylinux/8/cloud" >> $GITHUB_ENV
              ;;
            *)
              echo "LXC_IMAGE=images:${OS_NAME}/${OS_REL}/cloud" >> $GITHUB_ENV
              ;;
          esac
        fi

    - name: Download package
      uses: actions/download-artifact@v4
      with:
        name: ${{ inputs.artifact_prefix }}${{ steps.verify.outputs.pkg }}_${{ env.OS_NAME }}_${{ env.OS_REL }}_${{ matrix.target }}

    - name: Install Incus
      run: |
        # pre ubuntu-24.04
        sudo mkdir -p /etc/apt/keyrings/
        sudo curl -fsSL https://pkgs.zabbly.com/key.asc -o /etc/apt/keyrings/zabbly.asc
        sudo sh -c 'cat <<EOF > /etc/apt/sources.list.d/zabbly-incus-lts-6.0.sources
        Enabled: yes
        Types: deb
        URIs: https://pkgs.zabbly.com/incus/lts-6.0
        Suites: $(. /etc/os-release && echo ${VERSION_CODENAME})
        Components: main
        Architectures: $(dpkg --print-architecture)
        Signed-By: /etc/apt/keyrings/zabbly.asc

        EOF'
        sudo apt-get update
        sudo apt-get install -y incus

    # Fix iptables so that LXC containers can connect out after Docker sets FORWARD to DROP by default
    # https://discuss.linuxcontainers.org/t/lxd-losts-iptables-rules-with-docker/15045/6.
    # This wasn't needed with Ubuntu 20.04 but is needed with Ubuntu 22.04.
    - name: Fix firewall for LXD
      run: |
        # This issue seems to affect various newer O/S's and the fix seems to be harmless on older O/S's, so we don't
        # bother trying to be clever about O/S version based logic but just do this for all hosts.
        sudo iptables -I DOCKER-USER -i incusbr0 -j ACCEPT
        sudo iptables -I DOCKER-USER -o incusbr0 -j ACCEPT

    - name: Add current user to LXD group
      run: |
        sudo usermod --append --groups incus $(whoami)

    - name: Initialize LXD
      run: |
        sudo incus admin init --auto

    - name: Check LXD configuration
      run: |
        sudo incus info

    # Use of IPv6 sometimes prevents yum update being able to resolve mirrorlist.centos.org.
    - name: Disable LXD assignment of IPv6 addresses
      run: |
        sudo incus network set incusbr0 ipv6.address none

    - name: Launch LXC container
      run: |
        # security.nesting=true is needed to avoid error "Failed to set up mount namespacing: Permission denied" in a
        # Debian 10 container.
        sudo incus launch ${LXC_IMAGE} -c security.nesting=true -c environment.DEBIAN_FRONTEND=noninteractive testcon

    # Run package update and install man and sudo support (missing in some LXC/LXD O/S images) but first wait for
    # cloud-init to finish otherwise the network isn't yet ready. Don't use cloud-init status --wait as that isn't
    # supported on older O/S's like Ubuntu 16.04 and Debian 9. Use the sudo package provided configuration files
    # otherwise when using sudo we get an error that the root user isn't allowed to use sudo.
    - name: Prepare container
      run: |
        echo "Waiting for cloud-init.."
        SUCCESS=0
        for i in $(seq 1 60); do
          if sudo incus exec testcon -- ls -la /var/lib/cloud/data/result.json; then
            SUCCESS=1
            break
          else
          sleep 1s
          fi
        done

        # Log the cloud-init result file for diagnostic purposes
        sudo incus exec testcon -- cat /var/lib/cloud/data/result.json || true

        if [ $SUCCESS -eq 0 ]; then
          sudo incus exec testcon -- ls -la /var/lib/ || true
          sudo incus exec testcon -- ls -lR /var/lib/cloud || true
          sudo incus exec testcon -- ls -lR /usr/lib/systemd/system/ || true
          sudo incus exec testcon -- ls -lR /etc/cloud/ || true
          sudo incus exec testcon -- find /etc/cloud -type f -print -exec cat {} \; || true
          sudo incus exec testcon -- cloud-init status || true
          sudo incus exec testcon -- systemctl status cloud-init || true
          sudo incus exec testcon -- systemctl || true
          sudo incus exec testcon -- journalctl -u cloud-init || true
          sudo incus exec testcon -- journalctl || true
          sudo incus exec testcon -- find /var/log/ -type f -iname '*cloud*' -print -exec cat {} \;
          # try and continue anyway...
          echo "::warning::Unable to detect completion of cloud-init, subsequent steps may encounter unexpected failures. Check the logs just before this line."
        fi

        case ${OS_NAME} in
          debian|ubuntu)
            sudo incus exec testcon -- apt-get update
            sudo incus exec testcon -- apt-get install -y -o Dpkg::Options::="--force-confnew" apt-transport-https ca-certificates man sudo wget
            ;;
          centos|rockylinux)
            if [[ "${MATRIX_IMAGE}" == "centos:8" ]]; then
              # allow CentOS 8 to continue working now that it is EOL
              # see: https://stackoverflow.com/a/70930049
              sudo incus exec testcon -- find /etc/yum.repos.d/ -name 'CentOS-Linux-*' -exec sed -i -e 's|mirrorlist=|#mirrorlist=|g' -e 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' {} \;
            fi
            sudo incus exec testcon -- yum install -y man
            ;;
        esac

    - name: Copy the newly built package into the LXC container
      run: |
        case ${OS_NAME} in
          debian|ubuntu)
            DEB_FILE=$(ls -1 debian/*.deb)
            sudo incus file push ${DEB_FILE} testcon/tmp/
            echo "PKG_FILE=$(basename $DEB_FILE)" >> $GITHUB_ENV
            ;;
          centos|rockylinux)
            RPM_FILE=$(ls -1 generate-rpm/*.rpm)
            sudo incus file push ${RPM_FILE} testcon/tmp/
            echo "PKG_FILE=$(basename $RPM_FILE)" >> $GITHUB_ENV
            ;;
        esac

    - name: Install previously published package
      id: instprev
      if: ${{ steps.verify.outputs.mode == 'upgrade-from-published' }}
      run: |
        case ${OS_NAME} in
          debian|ubuntu)
            if [[ -f '${{ steps.verify.outputs.deb_apt_source }}' ]]; then
              sudo incus file push ${{ steps.verify.outputs.deb_apt_source }} testcon/etc/apt/sources.list.d/ploutos.list
            else
              echo -e '${{ steps.verify.outputs.deb_apt_source }}' >$HOME/ploutos.list
              sudo incus file push $HOME/ploutos.list testcon/etc/apt/sources.list.d/
            fi
            sudo incus exec testcon -- sed -i -e "s/\${OS_NAME}/${OS_NAME}/g" -e "s/\${OS_REL}/${OS_REL}/g" /etc/apt/sources.list.d/ploutos.list
            sudo incus exec testcon -- wget -q ${{ steps.verify.outputs.deb_apt_key_url }} -Oaptkey.asc
            sudo incus exec testcon -- apt-key add ./aptkey.asc
            sudo incus exec testcon -- apt-get -y update
            sudo incus exec testcon -- apt-get install -y ${{ steps.verify.outputs.published_pkg }}

            if [[ "${{ steps.verify.outputs.pkg }}" == "${{ steps.verify.outputs.published_pkg }}" ]]; then
              # determine the conffiles, append a harmless line break to each one (so that they are modified) then save
              # the md5sums of the modified files for comparison after upgrade to ensure our edits are not overwritten
              SAVED_MD5SUMS="/tmp/${{ steps.verify.outputs.pkg }}-conffiles.md5"
              CONFFILE_LIST_FILE="/var/lib/dpkg/info/${{ steps.verify.outputs.pkg }}.conffiles"
  
              # append a line break to the conffile
              for F in $(sudo incus exec testcon -- cat ${CONFFILE_LIST_FILE}); do
                sudo incus exec testcon -- sh -c "echo >> $F"
              done
  
              # save the md5 checksums for later comparison
              if sudo incus exec testcon -- sh -c "xargs -a ${CONFFILE_LIST_FILE} md5sum > ${SAVED_MD5SUMS}"; then
                sudo incus exec testcon -- cat ${SAVED_MD5SUMS}
              else
                echo "Conffile change preservation checking will be skipped because no conffiles were detected."
                sudo incus exec testcon -- rm -f ${SAVED_MD5SUMS}
              fi
            fi
            ;;
          centos|rockylinux)
            if [[ -f '${{ steps.verify.outputs.rpm_yum_repo }}' ]]; then
              sudo incus file push ${{ steps.verify.outputs.rpm_yum_repo }} testcon/etc/yum.repos.d/ploutos.repo
            else
              echo -e '${{ steps.verify.outputs.rpm_yum_repo }}' >$HOME/ploutos.repo
              sudo incus file push $HOME/ploutos.repo testcon/etc/yum.repos.d/
            fi
            sudo incus exec testcon -- sed -i -e "s/\${OS_NAME}/${OS_NAME}/g" -e "s/\${OS_REL}/${OS_REL}/g" /etc/yum.repos.d/ploutos.repo
            sudo incus exec testcon -- rpm --import ${{ steps.verify.outputs.rpm_yum_key_url }}
            sudo incus exec testcon -- yum install -y ${{ steps.verify.outputs.published_pkg }}
            ;;
        esac

    - name: Install the newly built package
      if: ${{ steps.verify.outputs.mode == 'fresh-install' }}
      run: |
        case ${OS_NAME} in
          debian|ubuntu)
            sudo incus exec testcon -- apt-get -y install /tmp/${PKG_FILE}
            ;;
          centos|rockylinux)
            sudo incus exec testcon -- yum install -y /tmp/${PKG_FILE} 2>&1 | tee install.log
            # yum install exits with code 0 even if scriptlets fail, so look for some sign of failure
            ! grep -qE '(err|warn|fail)' install.log
            ;;
        esac

    - name: Test the installed package
      if: ${{ inputs.package_test_scripts_path != '' && steps.verify.outputs.mode == 'fresh-install' }}
      run: |
        TEST_SCRIPT="$(echo '${{ inputs.package_test_scripts_path }}' | sed -e 's/<package>/${{ steps.verify.outputs.pkg }}/g')"
        sudo incus file push ${TEST_SCRIPT} testcon/tmp/test.sh
        sudo incus exec testcon -- chmod +x /tmp/test.sh
        sudo incus exec testcon -- /tmp/test.sh post-install

    - name: Upgrade from the published package to the newly built package
      id: upgrade_package
      continue-on-error: true
      if: ${{ steps.verify.outputs.mode == 'upgrade-from-published' }}
      run: |
        case ${OS_NAME} in
          debian|ubuntu)
            # See https://github.com/NLnetLabs/.github/issues/17 regarding --force-confXXX
            sudo incus exec testcon -- apt-get -y -o Dpkg::Options::="--force-confdef" -o Dpkg::Options::="--force-confold" install /tmp/${PKG_FILE}
            ;;
          centos|rockylinux)
            sudo incus exec testcon -- yum install -y /tmp/${PKG_FILE} 2>&1 | tee upgrade.log
            # yum install exits with code 0 even if scriptlets fail, so look for some sign of failure
            ! grep -qE '(err|warn|fail)' upgrade.log
            ;;
        esac

    - name: Require successful upgrade
      if: ${{ steps.upgrade_package.outcome == 'failure' && !matrix.ignore_upgrade_failure }}
      uses: actions/github-script@v7
      with:
        script: |
          core.setFailed("Package upgrade failed unexpectedly")

    - name: Verify that conffiles have not been altered by the upgrade
      if: ${{ steps.verify.outputs.mode == 'upgrade-from-published' }}
      run: |
        case ${OS_NAME} in
          debian|ubuntu)
            SAVED_MD5SUMS="/tmp/${{ steps.verify.outputs.pkg }}-conffiles.md5"
            sudo incus exec testcon -- sh -c "[ ! -f ${SAVED_MD5SUMS} ] || md5sum -c ${SAVED_MD5SUMS}"
            ;;
        esac

    - name: Test the upgraded package
      if: ${{ inputs.package_test_scripts_path != '' && steps.verify.outputs.mode == 'upgrade-from-published' }}
      run: |
        TEST_SCRIPT="$(echo '${{ inputs.package_test_scripts_path }}' | sed -e 's/<package>/${{ steps.verify.outputs.pkg }}/g')"
        sudo incus file push ${TEST_SCRIPT} testcon/tmp/test2.sh
        sudo incus exec testcon -- chmod +x /tmp/test2.sh
        sudo incus exec testcon -- /tmp/test2.sh post-upgrade

    - name: Test uninstall (without purge)
      run: |
        case ${OS_NAME} in
          debian|ubuntu)
            sudo incus exec testcon -- apt-get -y remove ${{ steps.verify.outputs.pkg }}
            ;;
          centos|rockylinux)
            sudo incus exec testcon -- yum remove -y ${{ steps.verify.outputs.pkg }} 2>&1 | tee remove.log
            # yum remove exits with code 0 even if scriptlets fail, so look for some sign of failure
            ! grep -qE '(err|warn|fail)' remove.log
            ;;
        esac

    - name: Test re-install
      run: |
        case ${OS_NAME} in
          debian|ubuntu)
            sudo incus exec testcon -- apt-get -y install /tmp/${PKG_FILE}
            ;;
          centos|rockylinux)
            sudo incus exec testcon -- yum install -y /tmp/${PKG_FILE} 2>&1 | tee reinstall.log
            # yum remove exits with code 0 even if scriptlets fail, so look for some sign of failure
            ! grep -qE '(err|warn|fail)' reinstall.log
            ;;
        esac


  # -------------------------------------------------------------------------------------------------------------------
  # Job: 'docker'
  # -------------------------------------------------------------------------------------------------------------------
  # Build and push architecture specific images (but NOT the main image that is used by end users). Sanity check the
  # image if possible (i.e. if it is x86-64, non-x86-64 architecture images cannot be run by this workflow yet). Logs
  # in to Docker Hub using secrets configured in this GitHub repository.
  #
  # NOTE: If you extend the set of matrix includes in this job you must also extend the call to docker manifest create
  # in the docker-manifest job below.
  docker:
    # Use of always() here ensures that even if the cross job is skipped we will still run
    if: ${{ always() && needs.prepare.outputs.docker_build_rules != '{}' }}
    needs: prepare
    outputs:
      published: ${{ steps.publish.conclusion == 'success' }}
    runs-on: ${{ inputs.runs_on }}
    strategy:
      fail-fast: false
      matrix: ${{ fromJSON(needs.prepare.outputs.docker_build_rules) }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Print matrix
        # Disable default use of bash -x for easier to read output in the log
        shell: bash
        run: |
          echo '${{ toJSON(matrix) }}'

      - name: Verify inputs
        id: verify
        run: |
          MODE="${{ matrix.mode }}"

          case ${MODE} in
            "")
              MODE="build"
              ;;

            build|copy)
              ;;

            *)
              echo "::error::Required matrix variable 'mode' in docker_build_rules must be one of 'copy' or 'build' (default)."
              exit 1
              ;;
          esac

          if [[ "${MODE}" == "copy" ]]; then
            if [[ "${{ matrix.platform }}" == "" ]]; then
              echo "::error::Matrix variable 'platform' in docker_build_rules must be a supported buildx platform. See: https://github.com/moby/buildkit#building-multi-platform-images"
              exit 1
            fi
          fi

          if [[ "${{ matrix.shortname }}" == "" ]]; then
            echo "::error::Matrix variable 'shortname' in docker_build_rules must set and non-empty."
            exit 1
          fi

          echo "mode=${MODE}" >> $GITHUB_OUTPUT

      - uses: docker/setup-qemu-action@v3
        # Don't use QEmu for compiling, it's way too slow on GitHub Actions.
        # Only use it for making images that will contain prebuilt binaries.
        if: ${{ steps.verify.outputs.mode == 'copy' }}
        with:
          platforms: ${{ matrix.platform }}

      - uses: docker/setup-buildx-action@v3
        with:
          version: v0.9.1 # See: https://github.com/docker/build-push-action/issues/755

      - name: Download cross-compiled binaries
        if: ${{ steps.verify.outputs.mode == 'copy' }}
        uses: NLnetLabs/download-artifact@main
        with:
          name: ${{ inputs.artifact_prefix }}tmp-cross-binaries-${{ matrix.target }}
          # The file downloaded to dockerbin/xxx will be used by the Dockerfile
          # Note: matrix.platform here has to match the Docker BuildX $TARGETPLATFORM variable available while building
          # the Dockerfile, e.g. linux/amd64.
          path: dockerbin/${{ matrix.platform }}
          maxTries: ${{ needs.prepare.outputs.cross_max_tries }}
          retryDelayMs: ${{ needs.prepare.outputs.cross_retry_delay_ms }}

      - name: Untar the set of downloaded binaries
        if: ${{ steps.verify.outputs.mode == 'copy' }}
        run: |
          tar vpxf dockerbin/${{ matrix.platform }}/bins.tar --transform='s/.*\///' -C dockerbin/${{ matrix.platform }}/
          find dockerbin/${{ matrix.platform }}/ -type d -empty -delete
          rm dockerbin/${{ matrix.platform }}/bins.tar

      - name: Generate architecture specific Docker image name
        id: gen
        run: |
          # Use the [0]the tag, i.e. `<repo>:unstable`, `<repo>:test` or `<repo>:v0.1.2`.
          # Don't use the [1]th tag (if present) which would be '<repo>:latest', as we won't build or publish an
          # image for 'latest' but instead push a multi-arch manifest for 'latest' which refers to the same
          # set of architecture specific images as the version specific multi-arch manifest we will publish.
          DOCKER_REPO_AND_TAG=$(echo "${{ needs.prepare.outputs.first_repo_and_tag_pair }}" | base64 -d)
          LOWER_DOCKER_ORG=$(echo "${{ needs.prepare.outputs.lower_docker_org }}" | base64 -d)
          echo "image_name=${LOWER_DOCKER_ORG}/${DOCKER_REPO_AND_TAG}-${{ matrix.shortname }}" >> $GITHUB_OUTPUT

      # Build a single architecture specific Docker image with an explicit architecture extension in the Docker
      # tag value. We have to push it to Docker Hub otherwise we can't make the multi-arch manifest below. If the
      # image fails testing (or doesn't work but wasn't caught because it is non-x86-64 which we can't at the moment
      # test here) it will have been pushed to Docker Hub but is NOT the image we expect people to use, that is the
      # combined multi-arch image that lacks the architecture specific tag value extension and that will ONLY be
      # pushed if all architecture specific images build and (where supported) passt he sanity check below.
      - name: Build Docker image ${{ steps.gen.outputs.image_name }}
        uses: docker/build-push-action@v5
        with:
          context: ${{ inputs.docker_context_path }}
          file: ${{ inputs.docker_file_path }}
          platforms: ${{ matrix.platform }}
          tags: ${{ steps.gen.outputs.image_name }}
          build-args: |
            MODE=${{ steps.verify.outputs.mode }}
            CARGO_ARGS=${{ matrix.cargo_args }}
          load: true

      - name: Save Docker image locally
        run: |
          docker save -o /tmp/docker-${{ matrix.shortname }}-img.tar ${{ steps.gen.outputs.image_name }}

      # Do a basic sanity check of the created image using the test tag to select the image to run, but only if the
      # image is for the x86-64 architecture as we don't yet have a way to run non-x86-64 architecture images.
      - name: Sanity check (linux/amd64 images only)
        if: ${{ matrix.platform == 'linux/amd64' && inputs.docker_sanity_check_command != '' }}
        run: |
          docker run --rm ${{ steps.gen.outputs.image_name }} ${{ inputs.docker_sanity_check_command }}

      - name: Log into Docker Hub
        if: ${{ fromJSON(needs.prepare.outputs.has_docker_secrets) == true }}
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_HUB_ID }}
          password: ${{ secrets.DOCKER_HUB_TOKEN }}

      # Upload the Docker image as a GitHub Actions artifact, handy when not publishing or investigating a problem
      - name: Upload built image to GitHub Actions
        uses: NLnetLabs/upload-artifact@main
        with:
          name: ${{ inputs.artifact_prefix }}tmp-docker-image-${{ matrix.shortname }}
          path: /tmp/docker-${{ matrix.shortname }}-img.tar
          if-no-files-found: warn

      - name: Publish image to Docker Hub
        id: publish
        if: ${{ fromJSON(needs.prepare.outputs.has_docker_secrets) == true && (contains(github.ref, 'refs/tags/v') || github.ref == 'refs/heads/main') }}
        uses: docker/build-push-action@v5
        with:
          context: ${{ inputs.docker_context_path }}
          file: ${{ inputs.docker_file_path }}
          platforms: ${{ matrix.platform }}
          tags: ${{ steps.gen.outputs.image_name }}
          build-args: |
            MODE=${{ steps.verify.outputs.mode }}
            CARGO_ARGS=${{ matrix.cargo_args }}
          push: true


  # -------------------------------------------------------------------------------------------------------------------
  # Job: 'docker-manifest'
  # -------------------------------------------------------------------------------------------------------------------
  # Create a Docker multi-arch "manifest" referencing the individual already pushed architecture specific images on
  # Docker Hub and push the manifest to Docker Hub as our "main" application image Docker Hub that end users will use.
  # Logs in to Docker Hub using secrets configured in this GitHub repository.
  docker-manifest:
    needs: [prepare, docker]
    outputs:
      published: ${{ steps.publish.conclusion == 'success' }}
    # Use of always() here ensures that even if the _cross_ job (note: not the 'docker' job) is skipped we will still run.
    # I wouldn't expect this to be needed but since the cross job was made conditional we seem to need this.
    if: ${{ always() && needs.prepare.outputs.docker_build_rules != '{}' && fromJSON(needs.prepare.outputs.has_docker_secrets) == true && (contains(github.ref, 'refs/tags/v') || github.ref == 'refs/heads/main') }}
    runs-on: ${{ inputs.runs_on }}
    steps:
      - name: Log into Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_HUB_ID }}
          password: ${{ secrets.DOCKER_HUB_TOKEN }}

      - name: Decode encoded Docker repo name and tag
        id: decode
        run: |
          LOWER_DOCKER_ORG=$(echo ${{ needs.prepare.outputs.lower_docker_org }} | base64 -d)
          REPO_AND_TAG_PAIRS=$(echo ${{ needs.prepare.outputs.all_repo_and_tag_pairs }} | base64 -d)

          # Convert the line-break separated tag pairs into space separated tag pairs without a trailing line break.
          # This can then easily be used with a shell for loop in the steps below.
          REPO_AND_TAG_PAIRS=$(echo ${REPO_AND_TAG_PAIRS} | tr "\n" " " | tr -d "\n")

          echo "repo_and_tag_pairs=${REPO_AND_TAG_PAIRS}" >> $GITHUB_OUTPUT
          echo "lower_docker_org=${LOWER_DOCKER_ORG}" >> $GITHUB_OUTPUT

      - name: Create multi-arch manifest(s)
        run: |
          LOWER_DOCKER_ORG="${{ steps.decode.outputs.lower_docker_org }}"

          ARCH_SHORT_NAMES="${{ join(fromJSON(needs.prepare.outputs.docker_build_rules).include.*.shortname, ' ') }}"
          REFERENCED_IMAGES=""

          # Imagine that we are invoked with two tags: v1.0.1 and latest
          # The first time round the loop we make a manifest for v1.0.1 referencing the v1.0.1 images.
          # The second time round the loop we make a manifest for latest also referencing the v1.0.1 images.
          for REPO_AND_TAG in ${{ steps.decode.outputs.repo_and_tag_pairs }}; do
            if [[ "${REFERENCED_IMAGES}" == "" ]]; then
              for ARCH_SHORT_NAME in ${ARCH_SHORT_NAMES}; do
                REFERENCED_IMAGES="${REFERENCED_IMAGES} ${LOWER_DOCKER_ORG}/${REPO_AND_TAG}-${ARCH_SHORT_NAME} "
              done
            fi

            docker manifest create --amend ${LOWER_DOCKER_ORG}/${REPO_AND_TAG} ${REFERENCED_IMAGES}
          done

      - name: Publish multi-arch image manifest(s) to Docker Hub
        id: publish
        run: |
          LOWER_DOCKER_ORG="${{ steps.decode.outputs.lower_docker_org }}"
          for REPO_AND_TAG in ${{ steps.decode.outputs.repo_and_tag_pairs }}; do
            docker manifest push ${LOWER_DOCKER_ORG}/${REPO_AND_TAG}
          done
